<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ceph on Friends&#39; Life and Work Space</title>
    <link>http://YLShiJustFly.github.io/tags/ceph/</link>
    <description>Recent content in ceph on Friends&#39; Life and Work Space</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 10 Jun 2025 15:17:59 +0800</lastBuildDate><atom:link href="http://YLShiJustFly.github.io/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Analysis of IO Commit Latency Spike in Ceph Cluster</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/analysis-of-io-commit-latency-spike-in-ceph-cluster/</link>
      <pubDate>Tue, 10 Jun 2025 15:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/analysis-of-io-commit-latency-spike-in-ceph-cluster/</guid>
      <description>Symptom Environment: After abnormal node reboot in Ceph cluster Affected Metric: Prometheus rate value of ceph_osd_op_w_latency Behavior: Pre-reboot: Values showed normal increment (peak ~1M) Post-reboot: Started recording from 0 Spiked to 4.2B after 3 minutes (close to 2³²) Investigation Process Phase 1: Initial Hypotheses Hypothesis Verification Method Conclusion Prometheus calculation Reviewed rate() function Confirmed proper counter reset Ceph stat initialization Inspected OSD.cc init code Verified proper atomic init Phase 2: Deep Analysis Key Findings:</description>
    </item>
    
    <item>
      <title>Ceph集群IO提交延迟统计跳变问题分析</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph%E9%9B%86%E7%BE%A4io%E6%8F%90%E4%BA%A4%E5%BB%B6%E8%BF%9F%E7%BB%9F%E8%AE%A1%E8%B7%B3%E5%8F%98%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</link>
      <pubDate>Tue, 10 Jun 2025 15:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph%E9%9B%86%E7%BE%A4io%E6%8F%90%E4%BA%A4%E5%BB%B6%E8%BF%9F%E7%BB%9F%E8%AE%A1%E8%B7%B3%E5%8F%98%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</guid>
      <description>问题现象 环境：Ceph集群节点异常重启后 异常指标：ceph_osd_op_w_latency的Prometheus rate值 具体表现： 重启前</description>
    </item>
    
    <item>
      <title>3FS的一点思考 </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/%E4%BB%8Edeepseek-3fs%E8%81%8A%E5%88%B0ai%E5%AD%98%E5%82%A8/</link>
      <pubDate>Fri, 30 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/%E4%BB%8Edeepseek-3fs%E8%81%8A%E5%88%B0ai%E5%AD%98%E5%82%A8/</guid>
      <description>横空出世: 3FS 近两个月，DeepSeek的热度值爆表。尤其令我们存储人欣喜的是，DeepSeek竟然把一向养在深闺无人识的分布式存储推到了台前</description>
    </item>
    
    <item>
      <title>Thinking about 3FS </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/from.deepseek-3fs.to.ai.storage/</link>
      <pubDate>Fri, 30 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/from.deepseek-3fs.to.ai.storage/</guid>
      <description>Rise to Prominence: 3FS In the past two months, DeepSeek&amp;rsquo;s popularity has skyrocketed. What particularly delights us storage professionals is that DeepSeek has brought distributed storage, which has long been hidden away, to the forefront.
The GitHub attention that 3FS has received is unprecedented among open-source distributed storage projects, and there probably won&amp;rsquo;t be another like it.
The star count in just 3 days after open-sourcing exceeded that of numerous open-source storage projects that have been cultivating for years.</description>
    </item>
    
    <item>
      <title>Ceph handle_cap_grant机制解析 </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/draft/kcephfs-caps/</link>
      <pubDate>Tue, 27 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/draft/kcephfs-caps/</guid>
      <description>handle_cap_grant handle_cap_grant 是 Ceph 分布式一致性机制的核心实现，负责： 处理 MDS 发送的能力授权/撤销消息 维护客户端缓存一致性 管理文件元数据同步 协调分布式锁机制 完整的流程图 1</description>
    </item>
    
  </channel>
</rss>
