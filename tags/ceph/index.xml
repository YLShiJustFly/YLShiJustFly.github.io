<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ceph on Friends&#39; Life and Work Space</title>
    <link>http://YLShiJustFly.github.io/tags/ceph/</link>
    <description>Recent content in ceph on Friends&#39; Life and Work Space</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 12 Oct 2025 02:59:34 +0000</lastBuildDate><atom:link href="http://YLShiJustFly.github.io/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>casps</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/draft/casps/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/draft/casps/</guid>
      <description>CephFS Caps æœºåˆ¶æ·±åº¦æŠ€æœ¯åˆ†æ ğŸ—ï¸ æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ CephFS çš„ capability (caps) æœºåˆ¶æ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ†å¸ƒå¼ä¸€è‡´æ€§ç³»ç»Ÿï¼Œç”¨äºç®¡ç†å®¢æˆ·ç«¯å¯¹æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡çš„è®¿é—®æƒé™ã€‚å®ƒç»“åˆäº†åˆ†å¸ƒå¼é”ã€ç¼“å­˜</description>
    </item>
    
    <item>
      <title>Ceph MDS (Metadata Server) æ¶æ„è§£æ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/ceph-mds-metadata-server-%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/ceph-mds-metadata-server-%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</guid>
      <description>MDS ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ Ceph MDSæ˜¯CephFS (Ceph File System) çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å¤„ç†æ‰€æœ‰æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®æ“ä½œã€‚MDSçš„è®¾è®¡é‡‡ç”¨åˆ†å¸ƒå¼ã€å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒå¤šæ´»MDSå’Œ</description>
    </item>
    
    <item>
      <title>ceph mgr-balanceræ¨¡å—æ‰§è¡Œæµç¨‹åˆ†æ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/zh-cn/ceph-mgr-balancer%E6%A8%A1%E5%9D%97%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/zh-cn/ceph-mgr-balancer%E6%A8%A1%E5%9D%97%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</guid>
      <description>éšç€OSDçš„æ›´æ›¿å’Œé›†ç¾¤çš„æ‰©ç¼©å®¹ï¼ŒPGåœ¨OSDçš„åˆ†å¸ƒä¼šé€æ¸å˜çš„ä¸å‡è¡¡ï¼Œå¯¼è‡´å„OSDçš„å®é™…å®¹é‡ä½¿ç”¨ç‡å‡ºç°å·®å¼‚ï¼Œé›†ç¾¤æ•´ä½“ä½¿ç”¨ç‡é™ä½ã€‚ceph bal</description>
    </item>
    
    <item>
      <title>Ceph Tentacleç‰ˆæœ¬çº åˆ ç å¢å¼º</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/ceph-tentacle%E7%89%88%E6%9C%AC%E7%BA%A0%E5%88%A0%E7%A0%81%E5%A2%9E%E5%BC%BA/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/ceph-tentacle%E7%89%88%E6%9C%AC%E7%BA%A0%E5%88%A0%E7%A0%81%E5%A2%9E%E5%BC%BA/</guid>
      <description>é¡¹ç›®æ„¿æ™¯ä¸ç›®æ ‡ æ ¸å¿ƒç›®æ ‡ï¼šä¼˜åŒ–çº åˆ ç ï¼ˆECï¼‰æ± çš„I/Oæ€§èƒ½ï¼Œä½¿å…¶æ¥è¿‘å¤åˆ¶æ± çš„æ€§èƒ½è¡¨ç° ä¸»è¦ç›®æ ‡ï¼š é™ä½æ€»ä½“æ‹¥æœ‰æˆæœ¬ï¼ˆTCOï¼‰ è®©çº åˆ ç æ± åœ¨å—å­˜å‚¨å’Œæ–‡</description>
    </item>
    
    <item>
      <title>CephFS-MDS System Architecture Overview</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/cephfs-mds-system-architecture-overview/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/cephfs-mds-system-architecture-overview/</guid>
      <description>MDS System Architecture Overview Ceph MDS is the core component of CephFS (Ceph File System), responsible for handling all file system metadata operations. MDS adopts a distributed, scalable architecture that supports multi-active MDS and dynamic load balancing.
MDS Position in Ceph Ecosystem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 graph TB Client[CephFS Client] --&amp;gt; MDS[MDS Cluster] MDS --&amp;gt; RADOS[RADOS Storage Layer] MDS --&amp;gt; Mon[Monitor Cluster] subgraph &amp;#34;MDS In Ceph&amp;#34; subgraph &amp;#34;Client Layer&amp;#34; Client Fuse[FUSE Client] Kernel[Kernel Client] end subgraph &amp;#34;Metadata Layer&amp;#34; MDS MDSStandby[Standby MDS] MDSActive[Active MDS] end subgraph &amp;#34;Storage Layer&amp;#34; RADOS OSD[OSD Cluster] Pool[Metadata Pool] end subgraph &amp;#34;Management Layer&amp;#34; Mon Mgr[Manager] end end Client -.</description>
    </item>
    
    <item>
      <title>cephé›†ç¾¤å·¡æ£€è„šæœ¬</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/zh-cn/ceph%E9%9B%86%E7%BE%A4%E5%B7%A1%E6%A3%80%E8%84%9A%E6%9C%AC/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/zh-cn/ceph%E9%9B%86%E7%BE%A4%E5%B7%A1%E6%A3%80%E8%84%9A%E6%9C%AC/</guid>
      <description>ğŸ› ï¸ è„šæœ¬åŠŸèƒ½ç‰¹ç‚¹ å…¨é¢çš„æ£€æŸ¥é¡¹ç›® âœ… é›†ç¾¤è¿æ¥çŠ¶æ€ - éªŒè¯Cephé›†ç¾¤å¯è¾¾æ€§ âœ… å¥åº·çŠ¶æ€åˆ†æ - HEALTH_OK/WARN/ERRè¯¦ç»†åˆ†æ âœ… Monit</description>
    </item>
    
    <item>
      <title>DAOS File System: Phoenix Rising After Optane&#39;s End</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/daos-file-system-phoenix-rising-after-optanes-end/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/daos-file-system-phoenix-rising-after-optanes-end/</guid>
      <description>Preface: Dominating the IO500 Rankings Having seen the DAOS project dominating the IO500 rankings, I&amp;rsquo;ve been keeping an eye on this project (though not diving deep into it). Background In previous articles, I briefly introduced the DAOS distributed storage project. However, with Intel terminating the Optane business in 2022, many people began to wonder: Can DAOS continue after losing its &amp;ldquo;core hardware support&amp;rdquo;? Where is its future?
Short-term Impact, but Not the End The discontinuation of Optane did have a significant impact on DAOS, especially in metadata acceleration and persistence.</description>
    </item>
    
    <item>
      <title>DAOSæ–‡ä»¶ç³»ç»Ÿï¼šåOptaneæ—¶ä»£çš„å‡¤å‡°æ¶…æ§ƒ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/daos%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%90%8Eoptane%E6%97%B6%E4%BB%A3%E7%9A%84%E5%87%A4%E5%87%B0%E6%B6%85%E6%A7%83/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/daos%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%90%8Eoptane%E6%97%B6%E4%BB%A3%E7%9A%84%E5%87%A4%E5%87%B0%E6%B6%85%E6%A7%83/</guid>
      <description>å‰è¨€ï¼šIO500ä¸Šçš„å± æ¦œ å› ä¸ºçœ‹åˆ°DAOSé¡¹ç›®åœ¨IO500ä¸Šçš„å± æ¦œï¼Œæ‰€ä»¥ä¸€ç›´å¯¹è¿™ä¸ªé¡¹ç›®ä¿æŒç€å…³æ³¨(æœªæ·±å…¥)ã€‚ èƒŒæ™¯ åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œç®€è¦ä»‹ç»äº†DA</description>
    </item>
    
    <item>
      <title>Execution Flow Analysis of the Ceph mgr-balancer Module </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/en/execution-flow-analysis-of-the-ceph-mgr-balancer-module-/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/en/execution-flow-analysis-of-the-ceph-mgr-balancer-module-/</guid>
      <description>As OSD are replaced and the cluster scales in and out, the distribution of PGs across OSDs becomes increasingly unbalanced. This leads to discrepancies in actual usage rates of individual OSDs, reducing the overall utilization rate of cluster. The ceph balancer module addresses this by adjusting weights or specifying PG mappings via upmap to redistribute PGs evently.
This article analyzes the execution flow when using balancer upmap mode, based on the ceph Pacific version.</description>
    </item>
    
    <item>
      <title>OSD</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/osd/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/osd/</guid>
      <description>Classical OSD Overall Architecture Diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 graph TB subgraph &amp;#34;OSD Process Architecture&amp;#34; OSD[OSD Main Class] OSDService[OSDService&amp;lt;br/&amp;gt;Core Service] ShardedOpWQ[ShardedOpWQ&amp;lt;br/&amp;gt;Sharded Operation Queue] Messenger[Messenger&amp;lt;br/&amp;gt;Message System] end subgraph &amp;#34;PG Management Subsystem&amp;#34; PGMap[pg_map&amp;lt;br/&amp;gt;PG Mapping Table] PG[PG Class&amp;lt;br/&amp;gt;Placement Group] PGBackend[PGBackend&amp;lt;br/&amp;gt;Backend Implementation] ReplicatedBackend[ReplicatedBackend] ECBackend[ECBackend] end subgraph &amp;#34;Object Storage Subsystem&amp;#34; ObjectStore[ObjectStore&amp;lt;br/&amp;gt;Storage Abstraction Layer] FileStore[FileStore&amp;lt;br/&amp;gt;Filesystem Storage] BlueStore[BlueStore&amp;lt;br/&amp;gt;Raw Device Storage] ObjectContext[ObjectContext&amp;lt;br/&amp;gt;Object Context] end subgraph &amp;#34;Recovery Subsystem&amp;#34; RecoveryState[RecoveryState&amp;lt;br/&amp;gt;Recovery State Machine] PeeringState[PeeringState&amp;lt;br/&amp;gt;Peering State] BackfillState[BackfillState&amp;lt;br/&amp;gt;Backfill State] RecoveryWQ[RecoveryWQ&amp;lt;br/&amp;gt;Recovery Work Queue] end subgraph &amp;#34;Monitoring &amp;amp; Statistics&amp;#34; PGStats[PGStats&amp;lt;br/&amp;gt;PG Statistics] OSDStats[OSDStats&amp;lt;br/&amp;gt;OSD Statistics] PerfCounters[PerfCounters&amp;lt;br/&amp;gt;Performance Counters] Logger[Logger&amp;lt;br/&amp;gt;Logging System] end OSD --&amp;gt; OSDService OSD --&amp;gt; ShardedOpWQ OSD --&amp;gt; Messenger OSD --&amp;gt; PGMap PGMap --&amp;gt; PG PG --&amp;gt; PGBackend PGBackend --&amp;gt; ReplicatedBackend PGBackend --&amp;gt; ECBackend PG --&amp;gt; ObjectStore ObjectStore --&amp;gt; FileStore ObjectStore --&amp;gt; BlueStore PG --&amp;gt; ObjectContext PG --&amp;gt; RecoveryState RecoveryState --&amp;gt; PeeringState RecoveryState --&amp;gt; BackfillState OSD --&amp;gt; RecoveryWQ PG --&amp;gt; PGStats OSD --&amp;gt; OSDStats OSD --&amp;gt; PerfCounters OSD --&amp;gt; Logger OSD Core Class Structure Details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 classDiagram class OSD { -int whoami -Messenger* cluster_messenger -Messenger* client_messenger -MonClient* monc -MgrClient* mgrc -ObjectStore* store -OSDService service -map~spg_t,PG*~ pg_map -RWLock pg_map_lock -OSDMapRef osdmap -epoch_t up_epoch -ThreadPool op_tp -ShardedOpWQ op_sharded_wq -RecoveryWQ recovery_wq -SnapTrimWQ snap_trim_wq -ScrubWQ scrub_wq +handle_osd_op(MOSDOp* op) +handle_replica_op(MOSDSubOp* op) +handle_pg_create(MOSDPGCreate* m) +handle_osd_map(MOSDMap* m) +process_peering_events() +start_boot() +shutdown() } class OSDService { -OSD* osd -CephContext* cct -ObjectStore* store -LogClient* log_client -PGRecoveryStats recovery_stats -Throttle recovery_ops_throttle -Throttle recovery_bytes_throttle -ClassHandler* class_handler -map~hobject_t,ObjectContext*~ object_contexts -LRUExpireMap object_context_lru +get_object_context(hobject_t oid) +release_object_context(ObjectContext* obc) +queue_for_recovery(PG* pg) +queue_for_scrub(PG* pg) } class ShardedOpWQ { -vector~OpWQ*~ shards -atomic~uint32_t~ next_shard +queue(OpRequestRef op) +dequeue(OpWQ* shard) +process_batch() } class OpWQ { -ThreadPool::TPHandle* handle -list~OpRequestRef~ ops -Mutex ops_lock +enqueue_front(OpRequestRef op) +enqueue_back(OpRequestRef op) +dequeue() +process() } OSD --&amp;gt; OSDService OSD --&amp;gt; ShardedOpWQ ShardedOpWQ --&amp;gt; OpWQ PG Class Detailed Structure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 classDiagram class PG { -spg_t pg_id -OSDService* osd -CephContext* cct -PGBackend* pgbackend -ObjectStore::CollectionHandle ch -RecoveryState recovery_state -PGLog pg_log -IndexedLog projected_log -eversion_t last_update -epoch_t last_epoch_started -set~pg_shard_t~ up -set~pg_shard_t~ acting -map~hobject_t,ObjectContext*~ object_contexts -Mutex pg_lock -Cond pg_cond -list~OpRequestRef~ waiting_for_peered -list~OpRequestRef~ waiting_for_active -map~eversion_t,list~OpRequestRef~~ waiting_for_ondisk +do_request(OpRequestRef op) +do_op(OpRequestRef op) +do_sub_op(OpRequestRef op) +execute_ctx(OpContext* ctx) +issue_repop(RepGather* repop) +eval_repop(RepGather* repop) +start_recovery_ops() +recover_object() +on_change(ObjectStore::Transaction* t) +activate() +clean_up_local() } class RecoveryState { -PG* pg -RecoveryMachine machine -boost::statechart::state_machine base +handle_event(const boost::statechart::event_base&amp;amp; evt) +process_peering_events() +advance_map() +need_up_thru() } class PGLog { -IndexedLog log -eversion_t tail -eversion_t head -list~pg_log_entry_t~ pending_log -set~eversion_t~ pending_dups +add(pg_log_entry_t&amp;amp; entry) +trim(eversion_t trim_to) +merge_log(ObjectStore::Transaction* t) +write_log_and_missing() } class PGBackend { -PG* parent -ObjectStore* store -CephContext* cct +submit_transaction() +objects_list_partial() +objects_list_range() +objects_get_attr() +objects_read_sync() +be_deep_scrub() } PG --&amp;gt; RecoveryState PG --&amp;gt; PGLog PG --&amp;gt; PGBackend Read/Write IO Processing Detailed Flow Write Operation Complete Flow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 sequenceDiagram participant Client participant OSD participant PG participant OpWQ participant ObjectStore participant Journal participant Replica Client-&amp;gt;&amp;gt;OSD: MOSDOp(write) OSD-&amp;gt;&amp;gt;OSD: handle_osd_op() Note right of OSD: 1.</description>
    </item>
    
    <item>
      <title>OSD</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/osd/</link>
      <pubDate>Sun, 12 Oct 2025 02:59:34 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/osd/</guid>
      <description>ç»å…¸OSDæ•´ä½“æ¶æ„å›¾ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 graph TB subgraph &amp;#34;</description>
    </item>
    
    <item>
      <title>Analysis of IO Commit Latency Spike in Ceph Cluster</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/analysis-of-io-commit-latency-spike-in-ceph-cluster/</link>
      <pubDate>Tue, 10 Jun 2025 15:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/analysis-of-io-commit-latency-spike-in-ceph-cluster/</guid>
      <description>Symptom Environment: After abnormal node reboot in Ceph cluster Affected Metric: Prometheus rate value of ceph_osd_op_w_latency Behavior: Pre-reboot: Values showed normal increment (peak ~1M) Post-reboot: Started recording from 0 Spiked to 4.2B after 3 minutes (close to 2Â³Â²) Investigation Process Phase 1: Initial Hypotheses Hypothesis Verification Method Conclusion Prometheus calculation Reviewed rate() function Confirmed proper counter reset Ceph stat initialization Inspected OSD.cc init code Verified proper atomic init Phase 2: Deep Analysis Key Findings: 3-minute zero period before spike 4.</description>
    </item>
    
    <item>
      <title>Cephé›†ç¾¤IOæäº¤å»¶è¿Ÿç»Ÿè®¡è·³å˜é—®é¢˜åˆ†æ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph%E9%9B%86%E7%BE%A4io%E6%8F%90%E4%BA%A4%E5%BB%B6%E8%BF%9F%E7%BB%9F%E8%AE%A1%E8%B7%B3%E5%8F%98%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</link>
      <pubDate>Tue, 10 Jun 2025 15:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph%E9%9B%86%E7%BE%A4io%E6%8F%90%E4%BA%A4%E5%BB%B6%E8%BF%9F%E7%BB%9F%E8%AE%A1%E8%B7%B3%E5%8F%98%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</guid>
      <description>é—®é¢˜ç°è±¡ ç¯å¢ƒï¼šCephé›†ç¾¤èŠ‚ç‚¹å¼‚å¸¸é‡å¯å å¼‚å¸¸æŒ‡æ ‡ï¼šceph_osd_op_w_latencyçš„Prometheus rateå€¼ å…·ä½“è¡¨ç°ï¼š é‡å¯å‰</description>
    </item>
    
    <item>
      <title>3FSçš„ä¸€ç‚¹æ€è€ƒ </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/%E4%BB%8Edeepseek-3fs%E8%81%8A%E5%88%B0ai%E5%AD%98%E5%82%A8/</link>
      <pubDate>Fri, 30 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/%E4%BB%8Edeepseek-3fs%E8%81%8A%E5%88%B0ai%E5%AD%98%E5%82%A8/</guid>
      <description>æ¨ªç©ºå‡ºä¸–: 3FS è¿‘ä¸¤ä¸ªæœˆï¼ŒDeepSeekçš„çƒ­åº¦å€¼çˆ†è¡¨ã€‚å°¤å…¶ä»¤æˆ‘ä»¬å­˜å‚¨äººæ¬£å–œçš„æ˜¯ï¼ŒDeepSeekç«Ÿç„¶æŠŠä¸€å‘å…»åœ¨æ·±é—ºæ— äººè¯†çš„åˆ†å¸ƒå¼å­˜å‚¨æ¨åˆ°äº†å°å‰</description>
    </item>
    
    <item>
      <title>Thinking about 3FS </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/from.deepseek-3fs.to.ai.storage/</link>
      <pubDate>Fri, 30 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/from.deepseek-3fs.to.ai.storage/</guid>
      <description>Rise to Prominence: 3FS In the past two months, DeepSeek&amp;rsquo;s popularity has skyrocketed. What particularly delights us storage professionals is that DeepSeek has brought distributed storage, which has long been hidden away, to the forefront. The GitHub attention that 3FS has received is unprecedented among open-source distributed storage projects, and there probably won&amp;rsquo;t be another like it. The star count in just 3 days after open-sourcing exceeded that of numerous open-source storage projects that have been cultivating for years.</description>
    </item>
    
    <item>
      <title>Ceph handle_cap_grantæœºåˆ¶è§£æ </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/draft/kcephfs-caps/</link>
      <pubDate>Tue, 27 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/draft/kcephfs-caps/</guid>
      <description>handle_cap_grant handle_cap_grant æ˜¯ Ceph åˆ†å¸ƒå¼ä¸€è‡´æ€§æœºåˆ¶çš„æ ¸å¿ƒå®ç°ï¼Œè´Ÿè´£ï¼š å¤„ç† MDS å‘é€çš„èƒ½åŠ›æˆæƒ/æ’¤é”€æ¶ˆæ¯ ç»´æŠ¤å®¢æˆ·ç«¯ç¼“å­˜ä¸€è‡´æ€§ ç®¡ç†æ–‡ä»¶å…ƒæ•°æ®åŒæ­¥ åè°ƒåˆ†å¸ƒå¼é”æœºåˆ¶ å®Œæ•´çš„æµç¨‹å›¾ 1</description>
    </item>
    
  </channel>
</rss>
