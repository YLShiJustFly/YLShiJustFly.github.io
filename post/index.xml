<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Friends&#39; Life and Work Space</title>
    <link>http://YLShiJustFly.github.io/post/</link>
    <description>Recent content in Posts on Friends&#39; Life and Work Space</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 03 Aug 2025 03:48:10 +0000</lastBuildDate><atom:link href="http://YLShiJustFly.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>casps</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/draft/casps/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/draft/casps/</guid>
      <description>CephFS Caps æœºåˆ¶æ·±åº¦æŠ€æœ¯åˆ†æ ğŸ—ï¸ æ ¸å¿ƒæ¶æ„æ¦‚è§ˆ CephFS çš„ capability (caps) æœºåˆ¶æ˜¯ä¸€ä¸ªå¤æ‚çš„åˆ†å¸ƒå¼ä¸€è‡´æ€§ç³»ç»Ÿï¼Œç”¨äºç®¡ç†å®¢æˆ·ç«¯å¯¹æ–‡ä»¶ç³»ç»Ÿå¯¹è±¡çš„è®¿é—®æƒé™ã€‚å®ƒç»“åˆäº†åˆ†å¸ƒå¼é”ã€ç¼“å­˜</description>
    </item>
    
    <item>
      <title>Ceph MDS (Metadata Server) æ¶æ„è§£æ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/ceph-mds-metadata-server-%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/ceph-mds-metadata-server-%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</guid>
      <description>MDS ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ Ceph MDSæ˜¯CephFS (Ceph File System) çš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£å¤„ç†æ‰€æœ‰æ–‡ä»¶ç³»ç»Ÿå…ƒæ•°æ®æ“ä½œã€‚MDSçš„è®¾è®¡é‡‡ç”¨åˆ†å¸ƒå¼ã€å¯æ‰©å±•çš„æ¶æ„ï¼Œæ”¯æŒå¤šæ´»MDSå’Œ</description>
    </item>
    
    <item>
      <title>ceph mgr-balanceræ¨¡å—æ‰§è¡Œæµç¨‹åˆ†æ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/zh-cn/ceph-mgr-balancer%E6%A8%A1%E5%9D%97%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/zh-cn/ceph-mgr-balancer%E6%A8%A1%E5%9D%97%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/</guid>
      <description>éšç€OSDçš„æ›´æ›¿å’Œé›†ç¾¤çš„æ‰©ç¼©å®¹ï¼ŒPGåœ¨OSDçš„åˆ†å¸ƒä¼šé€æ¸å˜çš„ä¸å‡è¡¡ï¼Œå¯¼è‡´å„OSDçš„å®é™…å®¹é‡ä½¿ç”¨ç‡å‡ºç°å·®å¼‚ï¼Œé›†ç¾¤æ•´ä½“ä½¿ç”¨ç‡é™ä½ã€‚ceph bal</description>
    </item>
    
    <item>
      <title>Ceph Tentacleç‰ˆæœ¬çº åˆ ç å¢å¼º</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/ceph-tentacle%E7%89%88%E6%9C%AC%E7%BA%A0%E5%88%A0%E7%A0%81%E5%A2%9E%E5%BC%BA/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/ceph-tentacle%E7%89%88%E6%9C%AC%E7%BA%A0%E5%88%A0%E7%A0%81%E5%A2%9E%E5%BC%BA/</guid>
      <description>é¡¹ç›®æ„¿æ™¯ä¸ç›®æ ‡ æ ¸å¿ƒç›®æ ‡ï¼šä¼˜åŒ–çº åˆ ç ï¼ˆECï¼‰æ± çš„I/Oæ€§èƒ½ï¼Œä½¿å…¶æ¥è¿‘å¤åˆ¶æ± çš„æ€§èƒ½è¡¨ç° ä¸»è¦ç›®æ ‡ï¼š é™ä½æ€»ä½“æ‹¥æœ‰æˆæœ¬ï¼ˆTCOï¼‰ è®©çº åˆ ç æ± åœ¨å—å­˜å‚¨å’Œæ–‡</description>
    </item>
    
    <item>
      <title>CephFS-MDS System Architecture Overview</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/cephfs-mds-system-architecture-overview/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/cephfs-mds-system-architecture-overview/</guid>
      <description>MDS System Architecture Overview Ceph MDS is the core component of CephFS (Ceph File System), responsible for handling all file system metadata operations. MDS adopts a distributed, scalable architecture that supports multi-active MDS and dynamic load balancing.
MDS Position in Ceph Ecosystem 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 graph TB Client[CephFS Client] --&amp;gt; MDS[MDS Cluster] MDS --&amp;gt; RADOS[RADOS Storage Layer] MDS --&amp;gt; Mon[Monitor Cluster] subgraph &amp;#34;MDS In Ceph&amp;#34; subgraph &amp;#34;Client Layer&amp;#34; Client Fuse[FUSE Client] Kernel[Kernel Client] end subgraph &amp;#34;Metadata Layer&amp;#34; MDS MDSStandby[Standby MDS] MDSActive[Active MDS] end subgraph &amp;#34;Storage Layer&amp;#34; RADOS OSD[OSD Cluster] Pool[Metadata Pool] end subgraph &amp;#34;Management Layer&amp;#34; Mon Mgr[Manager] end end Client -.</description>
    </item>
    
    <item>
      <title>cephé›†ç¾¤å·¡æ£€è„šæœ¬</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/zh-cn/ceph%E9%9B%86%E7%BE%A4%E5%B7%A1%E6%A3%80%E8%84%9A%E6%9C%AC/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/zh-cn/ceph%E9%9B%86%E7%BE%A4%E5%B7%A1%E6%A3%80%E8%84%9A%E6%9C%AC/</guid>
      <description>ğŸ› ï¸ è„šæœ¬åŠŸèƒ½ç‰¹ç‚¹ å…¨é¢çš„æ£€æŸ¥é¡¹ç›® âœ… é›†ç¾¤è¿æ¥çŠ¶æ€ - éªŒè¯Cephé›†ç¾¤å¯è¾¾æ€§ âœ… å¥åº·çŠ¶æ€åˆ†æ - HEALTH_OK/WARN/ERRè¯¦ç»†åˆ†æ âœ… Monit</description>
    </item>
    
    <item>
      <title>DAOS File System: Phoenix Rising After Optane&#39;s End</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/daos-file-system-phoenix-rising-after-optanes-end/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/daos-file-system-phoenix-rising-after-optanes-end/</guid>
      <description>Preface: Dominating the IO500 Rankings Having seen the DAOS project dominating the IO500 rankings, I&amp;rsquo;ve been keeping an eye on this project (though not diving deep into it). Background In previous articles, I briefly introduced the DAOS distributed storage project. However, with Intel terminating the Optane business in 2022, many people began to wonder: Can DAOS continue after losing its &amp;ldquo;core hardware support&amp;rdquo;? Where is its future?
Short-term Impact, but Not the End The discontinuation of Optane did have a significant impact on DAOS, especially in metadata acceleration and persistence.</description>
    </item>
    
    <item>
      <title>DAOSæ–‡ä»¶ç³»ç»Ÿï¼šåOptaneæ—¶ä»£çš„å‡¤å‡°æ¶…æ§ƒ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/daos%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%90%8Eoptane%E6%97%B6%E4%BB%A3%E7%9A%84%E5%87%A4%E5%87%B0%E6%B6%85%E6%A7%83/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/daos%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%90%8Eoptane%E6%97%B6%E4%BB%A3%E7%9A%84%E5%87%A4%E5%87%B0%E6%B6%85%E6%A7%83/</guid>
      <description>å‰è¨€ï¼šIO500ä¸Šçš„å± æ¦œ å› ä¸ºçœ‹åˆ°DAOSé¡¹ç›®åœ¨IO500ä¸Šçš„å± æ¦œï¼Œæ‰€ä»¥ä¸€ç›´å¯¹è¿™ä¸ªé¡¹ç›®ä¿æŒç€å…³æ³¨(æœªæ·±å…¥)ã€‚ èƒŒæ™¯ åœ¨ä¹‹å‰çš„æ–‡ç« ä¸­ï¼Œç®€è¦ä»‹ç»äº†DA</description>
    </item>
    
    <item>
      <title>Execution Flow Analysis of the Ceph mgr-balancer Module </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/en/execution-flow-analysis-of-the-ceph-mgr-balancer-module-/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/code-analysis/en/execution-flow-analysis-of-the-ceph-mgr-balancer-module-/</guid>
      <description>As OSD are replaced and the cluster scales in and out, the distribution of PGs across OSDs becomes increasingly unbalanced. This leads to discrepancies in actual usage rates of individual OSDs, reducing the overall utilization rate of cluster. The ceph balancer module addresses this by adjusting weights or specifying PG mappings via upmap to redistribute PGs evently.
This article analyzes the execution flow when using balancer upmap mode, based on the ceph Pacific version.</description>
    </item>
    
    <item>
      <title>OSD</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/osd/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/osd/</guid>
      <description>Classical OSD Overall Architecture Diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 graph TB subgraph &amp;#34;OSD Process Architecture&amp;#34; OSD[OSD Main Class] OSDService[OSDService&amp;lt;br/&amp;gt;Core Service] ShardedOpWQ[ShardedOpWQ&amp;lt;br/&amp;gt;Sharded Operation Queue] Messenger[Messenger&amp;lt;br/&amp;gt;Message System] end subgraph &amp;#34;PG Management Subsystem&amp;#34; PGMap[pg_map&amp;lt;br/&amp;gt;PG Mapping Table] PG[PG Class&amp;lt;br/&amp;gt;Placement Group] PGBackend[PGBackend&amp;lt;br/&amp;gt;Backend Implementation] ReplicatedBackend[ReplicatedBackend] ECBackend[ECBackend] end subgraph &amp;#34;Object Storage Subsystem&amp;#34; ObjectStore[ObjectStore&amp;lt;br/&amp;gt;Storage Abstraction Layer] FileStore[FileStore&amp;lt;br/&amp;gt;Filesystem Storage] BlueStore[BlueStore&amp;lt;br/&amp;gt;Raw Device Storage] ObjectContext[ObjectContext&amp;lt;br/&amp;gt;Object Context] end subgraph &amp;#34;Recovery Subsystem&amp;#34; RecoveryState[RecoveryState&amp;lt;br/&amp;gt;Recovery State Machine] PeeringState[PeeringState&amp;lt;br/&amp;gt;Peering State] BackfillState[BackfillState&amp;lt;br/&amp;gt;Backfill State] RecoveryWQ[RecoveryWQ&amp;lt;br/&amp;gt;Recovery Work Queue] end subgraph &amp;#34;Monitoring &amp;amp; Statistics&amp;#34; PGStats[PGStats&amp;lt;br/&amp;gt;PG Statistics] OSDStats[OSDStats&amp;lt;br/&amp;gt;OSD Statistics] PerfCounters[PerfCounters&amp;lt;br/&amp;gt;Performance Counters] Logger[Logger&amp;lt;br/&amp;gt;Logging System] end OSD --&amp;gt; OSDService OSD --&amp;gt; ShardedOpWQ OSD --&amp;gt; Messenger OSD --&amp;gt; PGMap PGMap --&amp;gt; PG PG --&amp;gt; PGBackend PGBackend --&amp;gt; ReplicatedBackend PGBackend --&amp;gt; ECBackend PG --&amp;gt; ObjectStore ObjectStore --&amp;gt; FileStore ObjectStore --&amp;gt; BlueStore PG --&amp;gt; ObjectContext PG --&amp;gt; RecoveryState RecoveryState --&amp;gt; PeeringState RecoveryState --&amp;gt; BackfillState OSD --&amp;gt; RecoveryWQ PG --&amp;gt; PGStats OSD --&amp;gt; OSDStats OSD --&amp;gt; PerfCounters OSD --&amp;gt; Logger OSD Core Class Structure Details 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 classDiagram class OSD { -int whoami -Messenger* cluster_messenger -Messenger* client_messenger -MonClient* monc -MgrClient* mgrc -ObjectStore* store -OSDService service -map~spg_t,PG*~ pg_map -RWLock pg_map_lock -OSDMapRef osdmap -epoch_t up_epoch -ThreadPool op_tp -ShardedOpWQ op_sharded_wq -RecoveryWQ recovery_wq -SnapTrimWQ snap_trim_wq -ScrubWQ scrub_wq +handle_osd_op(MOSDOp* op) +handle_replica_op(MOSDSubOp* op) +handle_pg_create(MOSDPGCreate* m) +handle_osd_map(MOSDMap* m) +process_peering_events() +start_boot() +shutdown() } class OSDService { -OSD* osd -CephContext* cct -ObjectStore* store -LogClient* log_client -PGRecoveryStats recovery_stats -Throttle recovery_ops_throttle -Throttle recovery_bytes_throttle -ClassHandler* class_handler -map~hobject_t,ObjectContext*~ object_contexts -LRUExpireMap object_context_lru +get_object_context(hobject_t oid) +release_object_context(ObjectContext* obc) +queue_for_recovery(PG* pg) +queue_for_scrub(PG* pg) } class ShardedOpWQ { -vector~OpWQ*~ shards -atomic~uint32_t~ next_shard +queue(OpRequestRef op) +dequeue(OpWQ* shard) +process_batch() } class OpWQ { -ThreadPool::TPHandle* handle -list~OpRequestRef~ ops -Mutex ops_lock +enqueue_front(OpRequestRef op) +enqueue_back(OpRequestRef op) +dequeue() +process() } OSD --&amp;gt; OSDService OSD --&amp;gt; ShardedOpWQ ShardedOpWQ --&amp;gt; OpWQ PG Class Detailed Structure 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 classDiagram class PG { -spg_t pg_id -OSDService* osd -CephContext* cct -PGBackend* pgbackend -ObjectStore::CollectionHandle ch -RecoveryState recovery_state -PGLog pg_log -IndexedLog projected_log -eversion_t last_update -epoch_t last_epoch_started -set~pg_shard_t~ up -set~pg_shard_t~ acting -map~hobject_t,ObjectContext*~ object_contexts -Mutex pg_lock -Cond pg_cond -list~OpRequestRef~ waiting_for_peered -list~OpRequestRef~ waiting_for_active -map~eversion_t,list~OpRequestRef~~ waiting_for_ondisk +do_request(OpRequestRef op) +do_op(OpRequestRef op) +do_sub_op(OpRequestRef op) +execute_ctx(OpContext* ctx) +issue_repop(RepGather* repop) +eval_repop(RepGather* repop) +start_recovery_ops() +recover_object() +on_change(ObjectStore::Transaction* t) +activate() +clean_up_local() } class RecoveryState { -PG* pg -RecoveryMachine machine -boost::statechart::state_machine base +handle_event(const boost::statechart::event_base&amp;amp; evt) +process_peering_events() +advance_map() +need_up_thru() } class PGLog { -IndexedLog log -eversion_t tail -eversion_t head -list~pg_log_entry_t~ pending_log -set~eversion_t~ pending_dups +add(pg_log_entry_t&amp;amp; entry) +trim(eversion_t trim_to) +merge_log(ObjectStore::Transaction* t) +write_log_and_missing() } class PGBackend { -PG* parent -ObjectStore* store -CephContext* cct +submit_transaction() +objects_list_partial() +objects_list_range() +objects_get_attr() +objects_read_sync() +be_deep_scrub() } PG --&amp;gt; RecoveryState PG --&amp;gt; PGLog PG --&amp;gt; PGBackend Read/Write IO Processing Detailed Flow Write Operation Complete Flow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 sequenceDiagram participant Client participant OSD participant PG participant OpWQ participant ObjectStore participant Journal participant Replica Client-&amp;gt;&amp;gt;OSD: MOSDOp(write) OSD-&amp;gt;&amp;gt;OSD: handle_osd_op() Note right of OSD: 1.</description>
    </item>
    
    <item>
      <title>OSD</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/osd/</link>
      <pubDate>Sun, 03 Aug 2025 03:48:10 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/osd/</guid>
      <description>ç»å…¸OSDæ•´ä½“æ¶æ„å›¾ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 graph TB subgraph &amp;#34;</description>
    </item>
    
    <item>
      <title>Analysis of IO Commit Latency Spike in Ceph Cluster</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/analysis-of-io-commit-latency-spike-in-ceph-cluster/</link>
      <pubDate>Tue, 10 Jun 2025 15:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/analysis-of-io-commit-latency-spike-in-ceph-cluster/</guid>
      <description>Symptom Environment: After abnormal node reboot in Ceph cluster Affected Metric: Prometheus rate value of ceph_osd_op_w_latency Behavior: Pre-reboot: Values showed normal increment (peak ~1M) Post-reboot: Started recording from 0 Spiked to 4.2B after 3 minutes (close to 2Â³Â²) Investigation Process Phase 1: Initial Hypotheses Hypothesis Verification Method Conclusion Prometheus calculation Reviewed rate() function Confirmed proper counter reset Ceph stat initialization Inspected OSD.cc init code Verified proper atomic init Phase 2: Deep Analysis Key Findings: 3-minute zero period before spike 4.</description>
    </item>
    
    <item>
      <title>Cephé›†ç¾¤IOæäº¤å»¶è¿Ÿç»Ÿè®¡è·³å˜é—®é¢˜åˆ†æ</title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph%E9%9B%86%E7%BE%A4io%E6%8F%90%E4%BA%A4%E5%BB%B6%E8%BF%9F%E7%BB%9F%E8%AE%A1%E8%B7%B3%E5%8F%98%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</link>
      <pubDate>Tue, 10 Jun 2025 15:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph%E9%9B%86%E7%BE%A4io%E6%8F%90%E4%BA%A4%E5%BB%B6%E8%BF%9F%E7%BB%9F%E8%AE%A1%E8%B7%B3%E5%8F%98%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</guid>
      <description>é—®é¢˜ç°è±¡ ç¯å¢ƒï¼šCephé›†ç¾¤èŠ‚ç‚¹å¼‚å¸¸é‡å¯å å¼‚å¸¸æŒ‡æ ‡ï¼šceph_osd_op_w_latencyçš„Prometheus rateå€¼ å…·ä½“è¡¨ç°ï¼š é‡å¯å‰</description>
    </item>
    
    <item>
      <title>3FSçš„ä¸€ç‚¹æ€è€ƒ </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/%E4%BB%8Edeepseek-3fs%E8%81%8A%E5%88%B0ai%E5%AD%98%E5%82%A8/</link>
      <pubDate>Fri, 30 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/%E4%BB%8Edeepseek-3fs%E8%81%8A%E5%88%B0ai%E5%AD%98%E5%82%A8/</guid>
      <description>æ¨ªç©ºå‡ºä¸–: 3FS è¿‘ä¸¤ä¸ªæœˆï¼ŒDeepSeekçš„çƒ­åº¦å€¼çˆ†è¡¨ã€‚å°¤å…¶ä»¤æˆ‘ä»¬å­˜å‚¨äººæ¬£å–œçš„æ˜¯ï¼ŒDeepSeekç«Ÿç„¶æŠŠä¸€å‘å…»åœ¨æ·±é—ºæ— äººè¯†çš„åˆ†å¸ƒå¼å­˜å‚¨æ¨åˆ°äº†å°å‰</description>
    </item>
    
    <item>
      <title>Thinking about 3FS </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/from.deepseek-3fs.to.ai.storage/</link>
      <pubDate>Fri, 30 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/from.deepseek-3fs.to.ai.storage/</guid>
      <description>Rise to Prominence: 3FS In the past two months, DeepSeek&amp;rsquo;s popularity has skyrocketed. What particularly delights us storage professionals is that DeepSeek has brought distributed storage, which has long been hidden away, to the forefront. The GitHub attention that 3FS has received is unprecedented among open-source distributed storage projects, and there probably won&amp;rsquo;t be another like it. The star count in just 3 days after open-sourcing exceeded that of numerous open-source storage projects that have been cultivating for years.</description>
    </item>
    
    <item>
      <title>Ceph handle_cap_grantæœºåˆ¶è§£æ </title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/draft/kcephfs-caps/</link>
      <pubDate>Tue, 27 May 2025 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/draft/kcephfs-caps/</guid>
      <description>handle_cap_grant handle_cap_grant æ˜¯ Ceph åˆ†å¸ƒå¼ä¸€è‡´æ€§æœºåˆ¶çš„æ ¸å¿ƒå®ç°ï¼Œè´Ÿè´£ï¼š å¤„ç† MDS å‘é€çš„èƒ½åŠ›æˆæƒ/æ’¤é”€æ¶ˆæ¯ ç»´æŠ¤å®¢æˆ·ç«¯ç¼“å­˜ä¸€è‡´æ€§ ç®¡ç†æ–‡ä»¶å…ƒæ•°æ®åŒæ­¥ åè°ƒåˆ†å¸ƒå¼é”æœºåˆ¶ å®Œæ•´çš„æµç¨‹å›¾ 1</description>
    </item>
    
    <item>
      <title>Obsidian &#43; Git pagesè‡ªåŠ¨éƒ¨ç½²åšå®¢ç³»ç»Ÿæ­å»º </title>
      <link>http://YLShiJustFly.github.io/post/obsidian&#43;git-pages%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Thu, 02 Mar 2023 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/obsidian&#43;git-pages%E8%87%AA%E5%8A%A8%E9%83%A8%E7%BD%B2%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%E6%90%AD%E5%BB%BA/</guid>
      <description>ä¸ºä»€ä¹ˆå†™è¿™ç¯‡æ–‡ç«  æˆ‘ä¹‹å‰ä½¿ç”¨ neovim å†™ markdownï¼Œhugo ç®¡ç†åšå®¢é™æ€ç«™ç‚¹ï¼Œå¹¶å†™äº†ä¸€ä¸ªç¼–è¯‘ hugoï¼Œå¹¶å‘å¸ƒåˆ° github pages çš„è„šæœ¬ã€‚ æœ€è¿‘å¼€å§‹è¿åˆ° obsidan åšç¬”è®°ï¼Œ</description>
    </item>
    
    <item>
      <title>vim&#43;markdown-preview.nvimæ­å»ºmarkdownå®æ—¶é¢„è§ˆå¼€å‘ç¯å¢ƒ</title>
      <link>http://YLShiJustFly.github.io/post/markdown/</link>
      <pubDate>Tue, 19 Apr 2022 10:39:58 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/markdown/</guid>
      <description>ä¸ºä»€ä¹ˆéœ€è¦markdown markdownæ˜¯ä¸€ç§è¯­æ³•æç®€å•çš„æ ‡è®°è¯­è¨€ï¼ŒåŸºäºmarkdownçš„æ–‡æœ¬ä»£ç å¯ä»¥å¾ˆæ–¹ä¾¿çš„æ¸²æŸ“æˆhtmlæ–‡ä»¶ï¼Œ åœ¨ç½‘ç«™ä¸Š</description>
    </item>
    
    <item>
      <title>macç‰ˆuniaccesså¸è½½æ–¹æ³•</title>
      <link>http://YLShiJustFly.github.io/post/uniaccess/</link>
      <pubDate>Tue, 22 Mar 2022 05:41:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/uniaccess/</guid>
      <description>ä¸ºä»€ä¹ˆè¦å¸è½½UniAcess? å¾ˆå¤šå…¬å¸ç”¨è”è½¯çš„UniAcessç›‘æ§å‘˜å·¥ç”µè„‘ï¼Œåå°è¿è¡Œçš„æ—¶å€™å ç”¨å¤§é‡CPUå’Œç½‘ç»œèµ„æºï¼Œ ä¸Šç½‘é€Ÿåº¦å—åˆ°å¾ˆå¤§å½±å“ï¼Œç”š</description>
    </item>
    
    <item>
      <title>Macosä¸‹å¸¸ç”¨æ•ˆç‡å·¥å…·</title>
      <link>http://YLShiJustFly.github.io/post/macos/</link>
      <pubDate>Mon, 21 Mar 2022 19:07:57 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/macos/</guid>
      <description>æˆ‘åœ¨ macOS ä¸‹å¸¸ç”¨åˆ°çš„æ•ˆç‡å·¥å…·ã€‚ é”®ç›˜æŒ‰é”®æ˜ å°„ Karabiner Elements&amp;mdash;å…è´¹ã€‚ ä½œä¸º mac ä¸‹åŠŸèƒ½æœ€å¼ºå¤§çš„æŒ‰é”®æ˜ å°„å·¥å…·ï¼Œå®ƒæ”¯æŒ simple æ¨¡å¼å’Œ complex æ¨¡å¼ï¼Œcomp</description>
    </item>
    
    <item>
      <title>vim&#43;plantuml-previewerç»˜åˆ¶æµç¨‹å›¾</title>
      <link>http://YLShiJustFly.github.io/post/plantuml/</link>
      <pubDate>Thu, 10 Mar 2022 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/plantuml/</guid>
      <description>ä¼ ç»Ÿç”»æµç¨‹å›¾çš„ç—›ç‚¹ æˆ‘ä»¬ç»å¸¸éœ€è¦ç”»æµç¨‹å›¾æ¥è¡¨ç¤ºä»£ç é€»è¾‘æˆ–è€…åŸºæœ¬æ¡†æ¶ç­‰ã€‚ä½†æˆ‘ä»¬åœ¨ç»˜ç”»æµç¨‹å›¾çš„æ—¶å€™ï¼Œç»å¸¸ä¼šåœ¨å¯¹é½è¿æ¥çº¿è¿™äº›å’Œæµç¨‹å›¾è¡¨è¾¾çš„æ„ä¹‰æ— å…³çš„</description>
    </item>
    
    <item>
      <title>ä½¿ç”¨vim&#43;vimtex&#43;latex(beamer) åˆ¶ä½œPPT</title>
      <link>http://YLShiJustFly.github.io/post/beamer/</link>
      <pubDate>Thu, 10 Mar 2022 14:17:59 +0800</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/beamer/</guid>
      <description>beamerå’Œpowerpointçš„ä¸åŒ æˆ‘ä»¬ç»å¸¸éœ€è¦ç”¨pptè¿™ä¸€å½¢å¼æ¥å±•ç¤ºæˆ‘ä»¬çš„å·¥ä½œæˆæœï¼Œä½†ä¼—æ‰€å‘¨çŸ¥ï¼Œå¾®è½¯çš„powerpointæ˜¯æ”¶è´¹è½¯ä»¶ï¼Œ</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/six-years-of-ceph-the-evolution-journey-from-nautilus-to-squid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/en/six-years-of-ceph-the-evolution-journey-from-nautilus-to-squid/</guid>
      <description>This document provides a detailed analysis of the major feature evolution in Ceph from Nautilus (v14) to the latest Squid (v19) versions, offering guidance for selecting appropriate versions and developing upgrade strategies. (Organized with LLM assistance)
Version Overview Version Codename Release Date Lifecycle Status v14.2.x Nautilus 2019 EOL v15.2.x Octopus 2020 EOL v16.2.x Pacific 2021 EOL v17.2.x Quincy 2022 EOL v18.2.x Reef 2023 Stable Maintenance v19.2.x Squid 2024 Current Stable Version Feature Comparison Summary Maturity-Driven Version Selection Guide Feature Nautilus Octopus Pacific Quincy Reef Squid Deployment Method ceph-deploy cephadm introduced cephadm cephadm mature cephadm cephadm Storage Engine BlueStore BlueStore BlueStore BlueStore FileStore removed BlueStore optimized Configuration Management Centralized introduced Centralized Centralized Centralized Centralized Centralized Network Protocol msgr2 introduced msgr2 stable msgr2 msgr2 msgr2 msgr2 PG Management autoscale introduced autoscale autoscale autoscale autoscale autoscale Scheduler Traditional Improved mclock introduced mclock default mclock mclock optimized CephFS Multi-FS First support Feature enhanced Mirroring perfected Management optimized Management optimized Dashboard integrated Multi-site Basic RBD mirroring CephFS mirroring Perfected Enhanced Enhanced Dashboard Basic Improved Improved Improved Refactored Refactored Containerization None cephadm preview cephadm mature cephadm complete cephadm cephadm Feature Maturity Marking Legend Bold text: Important milestones for features in this version (first introduction/stability achieved/major improvements) Normal text: Features remain stable or have minor improvements in this version Italic text: Features are deprecated or being prepared for removal in this version Feature-Driven Version Selection Guide Required Feature Minimum Version Stable Recommended Version Notes Centralized Configuration Management Nautilus Octopus+ Basic functionality available, upgrade recommended for stability PG autoscaling Nautilus Pacific+ Production environments recommend manual control msgr2 Security Protocol Nautilus Octopus+ Recommended for new deployments cephadm Container Management Octopus Pacific+ Tech preview â†’ production ready CephFS Multi-filesystem Nautilus Pacific+ Basic support â†’ production ready CephFS Mirroring/DR Octopus Pacific+ Feature introduction â†’ production stable mclock QoS Scheduling Pacific Quincy+ Introduction â†’ default enabled Full Containerized Deployment Pacific Quincy+ Basic support â†’ complete ecosystem Advanced Dashboard Reef Squid+ Refactored â†’ complete FileStore Replacement Any version Reef+ FileStore support removed after Reef Feature First Introduction and Maturity Analysis Important Feature Lifecycle Timeline This section details the first introduction version, stable version, and recommended production environment adoption timing for key features, helping users make informed version choices.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph-%E5%90%84%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7%E5%AF%B9%E6%AF%94%E4%B8%8E%E6%BC%94%E8%BF%9B%E5%88%86%E6%9E%90/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/application-practice/zh-cn/ceph-%E5%90%84%E7%89%88%E6%9C%AC%E7%89%B9%E6%80%A7%E5%AF%B9%E6%AF%94%E4%B8%8E%E6%BC%94%E8%BF%9B%E5%88%86%E6%9E%90/</guid>
      <description>Ceph å„ç‰ˆæœ¬ç‰¹æ€§å¯¹æ¯”ä¸æ¼”è¿›åˆ†æ æœ¬æ–‡æ¡£è¯¦ç»†åˆ†æäº†Cephä»Nautilus (v14)åˆ°æœ€æ–°Squid (v19)ç‰ˆæœ¬çš„ä¸»è¦ç‰¹æ€§æ¼”è¿›ï¼Œä¸ºé€‰æ‹©åˆé€‚ç‰ˆæœ¬å’Œ</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/ceph-mon-system-architecture-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/en/ceph-mon-system-architecture-overview/</guid>
      <description>Ceph Monitor Architecture Analysis Monitor Overall Architecture Overview Core Functional Positioning Ceph Monitor serves as the control plane of the cluster, primarily responsible for the following core duties:
Cluster Map Maintenance: Managing key mapping information including MonitorMap, OSDMap, CRUSHMap, MDSMap, PGMap, etc. Status Monitoring &amp;amp; Health Checks: Real-time monitoring of cluster status and generating health reports Distributed Consistency Guarantee: Ensuring cluster metadata consistency across all nodes based on Paxos algorithm Authentication &amp;amp; Authorization: Managing CephX authentication system and user permissions Election &amp;amp; Arbitration: Maintaining Monitor quorum and handling failure recovery Monitor Architecture Diagram 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 graph TB subgraph &amp;#34;Ceph Monitor Core Architecture&amp;#34; A[Monitor Daemon] --&amp;gt; B[MonitorStore] A --&amp;gt; C[Paxos Engine] A --&amp;gt; D[Election Module] A --&amp;gt; E[Health Module] A --&amp;gt; F[Config Module] A --&amp;gt; G[Auth Module] B --&amp;gt; B1[ClusterMap Storage] B --&amp;gt; B2[Configuration DB] B --&amp;gt; B3[Transaction Log] C --&amp;gt; C1[Proposal Processing] C --&amp;gt; C2[Leader Election] C --&amp;gt; C3[Consensus Coordination] D --&amp;gt; D1[Connectivity Strategy] D --&amp;gt; D2[Quorum Management] D --&amp;gt; D3[Split-brain Prevention] E --&amp;gt; E1[Health Checks] E --&amp;gt; E2[Status Reporting] E --&amp;gt; E3[Alert Generation] F --&amp;gt; F1[Config Key-Value Store] F --&amp;gt; F2[Runtime Configuration] F --&amp;gt; F3[Config Distribution] G --&amp;gt; G1[CephX Authentication] G --&amp;gt; G2[User Management] G --&amp;gt; G3[Capability Control] end subgraph &amp;#34;External Interactions&amp;#34; H[OSD Daemons] --&amp;gt; A I[MDS Daemons] --&amp;gt; A J[Client Applications] --&amp;gt; A K[Admin Tools] --&amp;gt; A L[Dashboard/Grafana] --&amp;gt; A end Monitor Core Submodule Analysis MonitorStore Storage Engine Functional Overview: MonitorStore is the persistent storage engine of Monitor, implemented based on RocksDB, responsible for storing all critical cluster metadata.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/ceph-mon%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/architecture-analysis/zh-cn/ceph-mon%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/</guid>
      <description>Ceph Monitor æ¶æ„è§£æ Monitoræ€»ä½“æ¶æ„æ¦‚è§ˆ æ ¸å¿ƒåŠŸèƒ½å®šä½ Ceph Monitorä½œä¸ºé›†ç¾¤çš„æ§åˆ¶å¹³é¢ï¼Œä¸»è¦æ‰¿æ‹…ä»¥ä¸‹æ ¸å¿ƒèŒè´£ï¼š é›†ç¾¤æ˜ å°„ç»´æŠ¤ï¼šç®¡ç†Monitor</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/ceph-tentacle-erasure-coding-enhancements/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/en/ceph-tentacle-erasure-coding-enhancements/</guid>
      <description>Vision and Objectives Core Goal: Optimize I/O performance for Erasure Coded pools to be similar to Replicated Pools Primary Objectives:
Lower Total Cost of Ownership (TCO) Make Erasure Coded pools viable for use with block and file storage Enabling &amp;ldquo;Optimised&amp;rdquo; EC Important Considerations Default State: All optimizations are turned off by default Per-Pool Configuration: Optimizations can be enabled for each pool individually âš ï¸ Irreversible Operation: OPTIMIZATIONS CANNOT BE SWITCHED OFF once enabled Version Requirements: All OSDs, MONs, and MGRs must be upgraded to Tentacle or later Backward Compatibility: Compatible with old clients Configuration Methods Enable optimizations for a specific pool 1 ceph osd pool set &amp;lt;pool_name&amp;gt; allow_ec_optimizations true Enable optimizations by default for new pools 1 2 [mon] osd_pool_default_flag_ec_optimizations = true Key Technical Features Previously Implemented Core Features Partial Reads Partial Writes Note: Partial metadata â€“ unwritten shards have no processing Parity Delta Writes Per-IO auto-switch between write methods Larger Default Chunk Size Direct Read Direct Write New Important Features 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/guide-for-distributed-file-syetems/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/distributed-storage/zh-cn/guide-for-distributed-file-syetems/</guid>
      <description>åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ (DFS) å®Œæ•´æŒ‡å— æ¥æº: WEKA - Distributed File Systems Guide æ—¥æœŸ: April 27, 2021 ç›®å½• DFSå‘½åç©ºé—´ åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿæ¶æ„ åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿç‰¹æ€§ ç°ä»£åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿçš„ç‰¹å¾ åˆ†å¸ƒå¼æ–‡</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/en/practical-guide-a-summary-of-commonly-used-tools-in-ceph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/en/practical-guide-a-summary-of-commonly-used-tools-in-ceph/</guid>
      <description>Practical Guide: Ceph Command Tools Summary ğŸ“‹ Common Tools (Summary Overview) Function Category Main Commands Verification Status Usage Frequency Application Scenarios Risk Level Cluster Monitoring ceph -s, ceph health(detail), ceph df, ceph -w âœ… Verified â­â­â­â­â­ Daily monitoring, troubleshooting ğŸŸ¢ No risk I/O Monitoring ceph iostat(version dependent,N+), ceph -w, ceph status âœ… Verified â­â­â­â­ Performance monitoring ğŸŸ¢ No risk OSD Management ceph osd tree, ceph osd status, ceph osd out/in âœ… Verified â­â­â­â­ OSD maintenance, capacity management ğŸŸ¡ Medium risk (queries safe) Monitor Management ceph mon stat, ceph quorum_status, ceph mon add/remove âœ… Verified â­â­â­ Cluster management, high availability ğŸ”´ High risk (queries safe) Manager Management ceph mgr module enable/disable, ceph mgr stat âœ… Verified â­â­â­ Feature management, dashboard ğŸŸ¡ Medium risk (queries safe) Pool Management ceph osd pool create/delete, ceph osd pool set âœ… Verified â­â­â­â­ Storage planning, quota management ğŸ”´ High risk (queries safe) PG Management ceph pg stat, ceph pg repair, ceph pg scrub âœ… Verified â­â­â­â­ Data integrity, fault repair ğŸŸ¡ Medium risk (queries safe) Authentication Management ceph auth list/create/del âœ… Verified â­â­â­ Security management, access control ğŸ”´ High risk (queries safe) CRUSH Management ceph osd crush tree, crushtool, ceph osd crush rule âœ… Verified â­â­ Data distribution, failure domains ğŸ”´ High risk (queries safe) RBD Management rbd create/rm, rbd snap create, rbd map/unmap âœ… Verified â­â­â­â­ Block storage, snapshot management ğŸŸ¡ Medium risk CephFS Management ceph fs status, ceph mds stat, ceph fs dump, ceph mds fail âœ… Verified â­â­â­ File system, metadata ğŸŸ¡ Medium risk (queries safe) RGW Management radosgw-admin user create, radosgw-admin bucket âœ… Verified â­â­â­ Object storage, user management ğŸŸ¡ Medium risk (queries safe) Configuration Management ceph config set/get, ceph tell Not verified â­â­â­â­ Parameter tuning, fault handling ğŸŸ¡ Medium risk (queries safe) Performance Analysis ceph osd perf,rbd perf image iostat, cephfs-top âœ… Verified â­â­â­ Performance testing, bottleneck analysis ğŸŸ¢ No risk Specialized Tools ceph-objectstore-tool, ceph-bluestore-tool âœ… Verified â­â­ Data recovery, deep diagnostics ğŸ”´ High risk (queries safe) Troubleshooting journalctl, ceph daemon dump, log analysis âœ… Verified â­â­â­â­ Problem diagnosis, root cause analysis ğŸŸ¢ No risk Backup Recovery ceph mon getmap, ceph auth export, data export âœ… Verified â­â­ Disaster recovery, migration ğŸŸ¡ Medium risk (queries safe) ğŸ”§ 1.</description>
    </item>
    
    <item>
      <title></title>
      <link>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/zh-cn/%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97ceph%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%B1%87%E6%80%BB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://YLShiJustFly.github.io/post/ceph-content/operation-skills/zh-cn/%E5%AE%9E%E7%94%A8%E6%8C%87%E5%8D%97ceph%E5%B8%B8%E7%94%A8%E5%B7%A5%E5%85%B7%E6%B1%87%E6%80%BB/</guid>
      <description>ğŸ“‹ å¸¸ç”¨å·¥å…·(æ±‡æ€»ç®€ç•¥ä¸å…¨ç‰ˆ) åŠŸèƒ½åˆ†ç±» ä¸»è¦å‘½ä»¤ éªŒè¯çŠ¶æ€ ä½¿ç”¨é¢‘ç‡ é€‚ç”¨åœºæ™¯ é£é™©çº§åˆ« é›†ç¾¤ç›‘æ§ ceph -s, ceph health(detail), ceph df, ceph -w âœ… å·²éªŒè¯ â­â­â­â­â­ æ—¥å¸¸ç›‘æ§ã€æ•…éšœè¯Šæ–­ ğŸŸ¢ æ— </description>
    </item>
    
  </channel>
</rss>
