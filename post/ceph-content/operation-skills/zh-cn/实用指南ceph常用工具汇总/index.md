

## ğŸ“‹ å¸¸ç”¨å·¥å…·(æ±‡æ€»ç®€ç•¥ä¸å…¨ç‰ˆ)

| åŠŸèƒ½åˆ†ç±» | ä¸»è¦å‘½ä»¤ | éªŒè¯çŠ¶æ€ | ä½¿ç”¨é¢‘ç‡ | é€‚ç”¨åœºæ™¯ | é£é™©çº§åˆ« |
|---------|---------|---------|---------|---------|---------|
| **é›†ç¾¤ç›‘æ§** | `ceph -s`, `ceph health(detail)`, `ceph df`, `ceph -w`| âœ… å·²éªŒè¯ | â­â­â­â­â­ | æ—¥å¸¸ç›‘æ§ã€æ•…éšœè¯Šæ–­ | ğŸŸ¢ æ— é£é™© |
| **I/Oç›‘æ§** | `ceph iostat`(ç‰ˆæœ¬ç›¸å…³,Nä»¥ä¸Š), `ceph -w`, `ceph status` | âœ… å·²éªŒè¯ | â­â­â­â­ | æ€§èƒ½ç›‘æ§ | ğŸŸ¢ æ— é£é™© |
| **OSDç®¡ç†** | `ceph osd tree`, `ceph osd status`, `ceph osd out/in` | âœ… å·²éªŒè¯ | â­â­â­â­ | OSDç»´æŠ¤ã€å®¹é‡ç®¡ç† | ğŸŸ¡ ä¸­ç­‰é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **Monitorç®¡ç†** | `ceph mon stat`, `ceph quorum_status`, `ceph mon add/remove` | âœ… å·²éªŒè¯ | â­â­â­ | é›†ç¾¤ç®¡ç†ã€é«˜å¯ç”¨ | ğŸ”´ é«˜é£é™© (æŸ¥è¯¢æ— é£é™©)|
| **Managerç®¡ç†** | `ceph mgr module enable/disable`, `ceph mgr stat` | âœ… å·²éªŒè¯ | â­â­â­ | åŠŸèƒ½ç®¡ç†ã€ä»ªè¡¨æ¿ | ğŸŸ¡ ä¸­ç­‰é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **å­˜å‚¨æ± ç®¡ç†** | `ceph osd pool create/delete`, `ceph osd pool set` | âœ… å·²éªŒè¯ | â­â­â­â­ | å­˜å‚¨è§„åˆ’ã€é…é¢ç®¡ç† | ğŸ”´ é«˜é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **PGç®¡ç†** | `ceph pg stat`, `ceph pg repair`, `ceph pg scrub` | âœ… å·²éªŒè¯ | â­â­â­â­ | æ•°æ®å®Œæ•´æ€§ã€æ•…éšœä¿®å¤ | ğŸŸ¡ ä¸­ç­‰é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **è®¤è¯ç®¡ç†** | `ceph auth list/create/del` | âœ… å·²éªŒè¯ | â­â­â­ | å®‰å…¨ç®¡ç†ã€æƒé™æ§åˆ¶ | ğŸ”´ é«˜é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **CRUSHç®¡ç†** | `ceph osd crush tree`, `crushtool`, `ceph osd crush rule` | âœ… å·²éªŒè¯ | â­â­ | æ•°æ®åˆ†å¸ƒã€æ•…éšœåŸŸ | ğŸ”´ é«˜é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **RBDç®¡ç†** | `rbd create/rm`, `rbd snap create`, `rbd map/unmap` | âœ… å·²éªŒè¯ | â­â­â­â­ | å—å­˜å‚¨ã€å¿«ç…§ç®¡ç† | ğŸŸ¡ ä¸­ç­‰é£é™© |
| **CephFSç®¡ç†** | `ceph fs status`, `ceph mds stat`, `ceph fs dump`, `ceph mds fail` | âœ… å·²éªŒè¯ | â­â­â­ | æ–‡ä»¶ç³»ç»Ÿã€å…ƒæ•°æ® | ğŸŸ¡ ä¸­ç­‰é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **RGWç®¡ç†** | `radosgw-admin user create`, `radosgw-admin bucket` | âœ… å·²éªŒè¯ | â­â­â­ | å¯¹è±¡å­˜å‚¨ã€ç”¨æˆ·ç®¡ç† | ğŸŸ¡ ä¸­ç­‰é£é™©  (æŸ¥è¯¢æ— é£é™©)|
| **é…ç½®ç®¡ç†** | `ceph config set/get`, `ceph tell` | æœªéªŒè¯ | â­â­â­â­ | å‚æ•°è°ƒä¼˜ã€æ•…éšœå¤„ç† | ğŸŸ¡ ä¸­ç­‰é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **æ€§èƒ½åˆ†æ** | `ceph osd perf`,`rbd perf image iostat`, cephfs-top | âœ… å·²éªŒè¯ | â­â­â­ | æ€§èƒ½æµ‹è¯•ã€ç“¶é¢ˆåˆ†æ | ğŸŸ¢ æ— é£é™© |
| **ä¸“ç”¨å·¥å…·** | `ceph-objectstore-tool`, `ceph-bluestore-tool` | âœ… å·²éªŒè¯ | â­â­ | æ•°æ®æ¢å¤ã€æ·±åº¦è¯Šæ–­ | ğŸ”´ é«˜é£é™© (æŸ¥è¯¢æ— é£é™©) |
| **æ•…éšœæ’æŸ¥** | `journalctl`, `ceph daemon dump`, æ—¥å¿—åˆ†æ | âœ… å·²éªŒè¯ | â­â­â­â­ | é—®é¢˜è¯Šæ–­ã€æ ¹å› åˆ†æ | ğŸŸ¢ æ— é£é™© |
| **å¤‡ä»½æ¢å¤** | `ceph mon getmap`, `ceph auth export`, æ•°æ®å¯¼å‡º | âœ… å·²éªŒè¯ | â­â­ | ç¾éš¾æ¢å¤ã€è¿ç§» | ğŸŸ¡ ä¸­ç­‰é£é™© (æŸ¥è¯¢æ— é£é™©) |




## ğŸ”§ 1. é›†ç¾¤çŠ¶æ€ç›‘æ§

### 1.1 é›†ç¾¤æ•´ä½“çŠ¶æ€
```bash
# æŸ¥çœ‹é›†ç¾¤çŠ¶æ€ï¼ˆæœ€å¸¸ç”¨ï¼‰
ceph status
ceph -s

# æŸ¥çœ‹é›†ç¾¤å¥åº·çŠ¶æ€
ceph health
ceph health detail

# å®æ—¶ç›‘æ§é›†ç¾¤çŠ¶æ€
ceph -w

# æŸ¥çœ‹é›†ç¾¤å­˜å‚¨ä½¿ç”¨æƒ…å†µ
ceph df
ceph df detail

# æŸ¥çœ‹é›†ç¾¤ç‰ˆæœ¬ä¿¡æ¯
ceph version
ceph versions
```

### 1.2 é›†ç¾¤æ€§èƒ½ç›‘æ§
```bash
# æŸ¥çœ‹é›†ç¾¤ I/O ç»Ÿè®¡
ceph iostat
watch "ceph iostat"

# æŸ¥çœ‹ OSD æ€§èƒ½ç»Ÿè®¡
ceph osd perf

# æŸ¥çœ‹æ…¢è¯·æ±‚
ceph daemon osd.X dump_slow_requests
ceph daemon osd.X dump_historic_slow_ops

# PG åˆ†å¸ƒåˆ†æ
ceph pg dump | grep ^pg | awk '{print $1,$15}' | sort
```

---

## ğŸ—„ï¸ 2. OSD ç®¡ç†

### 2.1 OSD åŸºæœ¬æ“ä½œ
```bash
# æŸ¥çœ‹ OSD çŠ¶æ€
ceph osd stat
ceph osd dump
ceph osd tree

# æŸ¥çœ‹ OSD ä½¿ç”¨æƒ…å†µ
ceph osd df
ceph osd df tree

# å¯åŠ¨/åœæ­¢ OSD
systemctl start ceph-osd@X
systemctl stop ceph-osd@X
systemctl restart ceph-osd@X

# å°† OSD æ ‡è®°ä¸º out/in
ceph osd out X
ceph osd in X

# å°† OSD æ ‡è®°ä¸º down/up
ceph osd down X
ceph osd up X
```

### 2.2 OSD ç»´æŠ¤æ“ä½œ
```bash
# å®‰å…¨ç§»é™¤ OSDï¼ˆå®Œæ•´æµç¨‹ï¼‰
ceph osd out X                              # æ ‡è®°ä¸ºoutï¼Œå¼€å§‹æ•°æ®è¿ç§»
ceph osd safe-to-destroy X                  # æ£€æŸ¥æ˜¯å¦å®‰å…¨ç§»é™¤
ceph osd destroy X --yes-i-really-mean-it   # é”€æ¯OSD
ceph osd crush remove osd.X                 # ä»CRUSH mapç§»é™¤
ceph auth del osd.X                         # åˆ é™¤è®¤è¯ä¿¡æ¯
ceph osd rm X                               # ä»é›†ç¾¤ç§»é™¤


# æ›¿æ¢æ•…éšœ OSD
ceph osd destroy X --yes-i-really-mean-it


# é‡æ–°æƒè¡¡ OSD
ceph osd reweight X 0.8                     # ä¸´æ—¶æƒé‡è°ƒæ•´
ceph osd crush reweight osd.X 2.0           # æ°¸ä¹…æƒé‡è°ƒæ•´
ceph osd reweight-by-utilization            # æŒ‰ä½¿ç”¨ç‡è‡ªåŠ¨è°ƒæ•´æƒé‡
```

### 2.3 OSD æ•…éšœæ’æŸ¥
```bash
# æŸ¥çœ‹ OSD è¯¦ç»†ä¿¡æ¯
ceph osd find X
ceph osd metadata X

# æŸ¥çœ‹ OSD æ—¥å¿—
journalctl -u ceph-osd@X -f
journalctl -u ceph-osd@X --since "1 hour ago"

# ä¿®å¤ OSD
ceph osd scrub X
ceph osd deep-scrub X

# æŸ¥çœ‹ OSD çš„ PG
ceph pg ls-by-osd X

# OSD æ€§èƒ½è¯Šæ–­
ceph daemon osd.X perf dump
ceph daemon osd.X dump_ops_in_flight
ceph daemon osd.X dump_blocked_ops
```

---

## ğŸ›ï¸ 3. Monitor ç®¡ç†

### 3.1 Monitor åŸºæœ¬æ“ä½œ
```bash
# æŸ¥çœ‹ Monitor çŠ¶æ€
ceph mon stat
ceph mon dump

# æŸ¥çœ‹ Monitor ä»²è£çŠ¶æ€
ceph quorum_status

# æ·»åŠ  Monitor
ceph mon add <mon-id> <mon-addr>

# ç§»é™¤ Monitor
ceph mon remove <mon-id>

# æŸ¥çœ‹ Monitor æ˜ å°„
ceph mon getmap -o /tmp/monmap
monmaptool --print /tmp/monmap
```

### 3.2 Monitor ç»´æŠ¤
```bash
# å¯åŠ¨/åœæ­¢ Monitor
systemctl start ceph-mon@<hostname>
systemctl stop ceph-mon@<hostname>

# å‹ç¼© Monitor æ•°æ®åº“
ceph tell mon.<mon-id> compact

# åŒæ­¥ Monitor æ—¶é—´
ceph time-sync-status

# Monitor æ•…éšœæ¢å¤
ceph-mon --extract-monmap /tmp/monmap --mon-data /var/lib/ceph/mon/ceph-<id>
```

---

## ğŸ‘¨â€ğŸ’¼ 4. Manager ç®¡ç†

### 4.1 Manager åŸºæœ¬æ“ä½œ
```bash
# æŸ¥çœ‹ Manager çŠ¶æ€
ceph mgr stat
ceph mgr dump

# å¯ç”¨/ç¦ç”¨ Manager æ¨¡å—
ceph mgr module enable <module>
ceph mgr module disable <module>

# æŸ¥çœ‹å¯ç”¨æ¨¡å—
ceph mgr module ls

# æ•…éšœè½¬ç§» Manager
ceph mgr fail <mgr-id>
```

### 4.2 å¸¸ç”¨ Manager æ¨¡å—
```bash
# Dashboard æ¨¡å—
ceph mgr module enable dashboard
ceph dashboard create-self-signed-cert
ceph dashboard ac-user-create <username> -i <password-file>
ceph dashboard ac-role-add-scope-perms admin read-write pool
ceph config set mgr mgr/dashboard/server_addr 0.0.0.0
ceph config set mgr mgr/dashboard/server_port 8443

# Prometheus æ¨¡å—
ceph mgr module enable prometheus
ceph config set mgr mgr/prometheus/server_addr 0.0.0.0
ceph config set mgr mgr/prometheus/server_port 9283

# Balancer æ¨¡å—
ceph mgr module enable balancer
ceph balancer mode upmap
ceph balancer on
ceph balancer status

# Alerts æ¨¡å—
ceph mgr module enable alerts
```

---

## ğŸ—‚ï¸ 5. å­˜å‚¨æ± ç®¡ç†

### 5.1 Pool åŸºæœ¬æ“ä½œ
```bash
# åˆ›å»ºå­˜å‚¨æ± 
ceph osd pool create <poolname> <pg_num> [pgp_num]
ceph osd pool create rbd 128 128 replicated
ceph osd pool create mypool 64 64 erasure

# åˆ é™¤å­˜å‚¨æ± 
ceph config set mon mon_allow_pool_delete true
ceph osd pool delete <poolname> <poolname> --yes-i-really-really-mean-it

# æŸ¥çœ‹å­˜å‚¨æ± 
ceph osd lspools
ceph osd pool ls detail

# è®¾ç½®å­˜å‚¨æ± é…é¢
ceph osd pool set-quota <poolname> max_objects 1000
ceph osd pool set-quota <poolname> max_bytes 1TB

# æŸ¥çœ‹å­˜å‚¨æ± ç»Ÿè®¡
ceph osd pool stats
ceph osd pool stats <poolname>
```

### 5.2 Pool å‚æ•°é…ç½®
```bash
# è®¾ç½®å­˜å‚¨æ± å‰¯æœ¬æ•°
ceph osd pool set <poolname> size 3
ceph osd pool set <poolname> min_size 2

# è®¾ç½® PG æ•°é‡
ceph osd pool set <poolname> pg_num 128
ceph osd pool set <poolname> pgp_num 128

# è®¾ç½®å­˜å‚¨æ± ç±»å‹
ceph osd pool set <poolname> crush_rule <rule-name>

# å¯ç”¨/ç¦ç”¨åŠŸèƒ½
ceph osd pool set <poolname> noscrub true
ceph osd pool set <poolname> nodeep-scrub true

# Pool åº”ç”¨æ ‡ç­¾
ceph osd pool application enable <poolname> rbd
ceph osd pool application enable <poolname> cephfs
ceph osd pool application enable <poolname> rgw
```

---

## ğŸ“„ 6. æ”¾ç½®ç»„ (PG) ç®¡ç†

### 6.1 PG çŠ¶æ€æŸ¥çœ‹
```bash
# æŸ¥çœ‹ PG çŠ¶æ€
ceph pg stat
ceph pg dump
ceph pg dump --format json-pretty

# æŸ¥çœ‹å¼‚å¸¸ PG
ceph pg dump_stuck
ceph pg dump_stuck inactive
ceph pg dump_stuck unclean

# æŸ¥çœ‹ç‰¹å®š PG ä¿¡æ¯
ceph pg <pgid> query
ceph pg map <pgid>
```

### 6.2 PG ä¿®å¤æ“ä½œ
```bash
# ä¿®å¤ PG
ceph pg repair <pgid>
ceph pg scrub <pgid>
ceph pg deep-scrub <pgid>

# å¼ºåˆ¶ä¿®å¤ä¸ä¸€è‡´çš„å¯¹è±¡
ceph pg force-recovery <pgid>
ceph pg force-backfill <pgid>

# æŸ¥çœ‹ PG çš„ OSD
ceph pg ls-by-pool <poolname>
ceph pg ls-by-primary <osd-id>

# PG è‡ªåŠ¨ä¿®å¤
ceph config set osd osd_scrub_auto_repair true
```

---

## ğŸ” 7. è®¤è¯ç®¡ç†

### 7.1 ç”¨æˆ·ç®¡ç†
```bash
# æŸ¥çœ‹è®¤è¯ä¿¡æ¯
ceph auth list
ceph auth get client.admin

# åˆ›å»ºç”¨æˆ·
ceph auth get-or-create client.<name> mon 'allow r' osd 'allow rw pool=<poolname>'
ceph auth get-or-create client.rbd mon 'profile rbd' osd 'profile rbd pool=rbd'

# åˆ é™¤ç”¨æˆ·
ceph auth del client.<name>

# å¯¼å‡º/å¯¼å…¥å¯†é’¥
ceph auth export client.<name> > client.<name>.keyring
ceph auth import -i client.<name>.keyring
```

### 7.2 æƒé™ç®¡ç†
```bash
# ä¿®æ”¹ç”¨æˆ·æƒé™
ceph auth caps client.<name> mon 'allow r' osd 'allow rw pool=rbd'

# æŸ¥çœ‹æƒé™
ceph auth get client.<name>

# ç”Ÿæˆå®¢æˆ·ç«¯é…ç½®
ceph config generate-minimal-conf

# å¸¸ç”¨æƒé™æ¨¡æ¿
# åªè¯»ç”¨æˆ·ï¼šmon 'allow r' osd 'allow r'
# RBDç”¨æˆ·ï¼šmon 'profile rbd' osd 'profile rbd pool=rbd'
# CephFSç”¨æˆ·ï¼šmon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs_data'
```

---

## ğŸ—ï¸ 8. CRUSH Map ç®¡ç†

### 8.1 CRUSH åŸºæœ¬æ“ä½œ
```bash
# æŸ¥çœ‹ CRUSH map
ceph osd tree
ceph osd crush tree

# å¯¼å‡º/å¯¼å…¥ CRUSH map
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt
crushtool -c crushmap.txt -o crushmap-new.bin
ceph osd setcrushmap -i crushmap-new.bin

# åˆ›å»º/åˆ é™¤ bucket
ceph osd crush add-bucket <bucket-name> <bucket-type>
ceph osd crush remove <bucket-name>

# ç§»åŠ¨ OSD åˆ°ä¸åŒä½ç½®
ceph osd crush move osd.X host=<hostname>
ceph osd crush move osd.X rack=<rackname>
```

### 8.2 CRUSH è§„åˆ™ç®¡ç†
```bash
# æŸ¥çœ‹ CRUSH è§„åˆ™
ceph osd crush rule ls
ceph osd crush rule dump

# åˆ›å»º CRUSH è§„åˆ™
ceph osd crush rule create-simple <rule-name> <root> <type>
ceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <device-class>

# åˆ é™¤ CRUSH è§„åˆ™
ceph osd crush rule rm <rule-name>

# æµ‹è¯• CRUSH æ˜ å°„
crushtool -i crushmap.bin --test --show-mappings --ruleset 0 --num-rep 3 --min-x 0 --max-x 100
```

---

## ğŸ¯ 9. RBD ç®¡ç†

### 9.1 RBD åŸºæœ¬æ“ä½œ
```bash
# åˆ›å»º RBD é•œåƒ
rbd create --size 1024 <poolname>/<image-name>
rbd create --size 10G --image-feature layering,exclusive-lock mypool/myimage

# æŸ¥çœ‹ RBD é•œåƒ
rbd ls <poolname>
rbd info <poolname>/<image-name>
rbd du <poolname>/<image-name>

# åˆ é™¤ RBD é•œåƒ
rbd rm <poolname>/<image-name>
rbd trash mv <poolname>/<image-name>    # ç§»åŠ¨åˆ°å›æ”¶ç«™
rbd trash restore <poolname>/<image-id>  # ä»å›æ”¶ç«™æ¢å¤

# è°ƒæ•´é•œåƒå¤§å°
rbd resize --size 2048 <poolname>/<image-name>
rbd resize --size 2G <poolname>/<image-name>

# åˆ›å»ºå¿«ç…§
rbd snap create <poolname>/<image-name>@<snap-name>
rbd snap ls <poolname>/<image-name>
rbd snap rm <poolname>/<image-name>@<snap-name>
```

### 9.2 RBD é«˜çº§æ“ä½œ
```bash
# å…‹éš†é•œåƒ
rbd snap protect <poolname>/<parent-image>@<snap-name>
rbd clone <poolname>/<parent-image>@<snap-name> <poolname>/<child-image>

# å±•å¹³å…‹éš†é•œåƒ
rbd flatten <poolname>/<image-name>

# å¯¼å‡º/å¯¼å…¥é•œåƒ
rbd export <poolname>/<image-name> image.raw
rbd export --export-format 2 <poolname>/<image-name> image.raw
rbd import image.raw <poolname>/<image-name>

# é•œåƒæ˜ å°„
rbd map <poolname>/<image-name>
rbd unmap /dev/rbdX
rbd showmapped

# é•œåƒç‰¹æ€§ç®¡ç†
rbd feature enable <poolname>/<image-name> exclusive-lock
rbd feature disable <poolname>/<image-name> deep-flatten
```

---

## ğŸŒ 10. CephFS ç®¡ç†

### 10.1 CephFS åŸºæœ¬æ“ä½œ
```bash
# åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ
ceph fs new <fs-name> <metadata-pool> <data-pool>

# æŸ¥çœ‹æ–‡ä»¶ç³»ç»Ÿ
ceph fs ls
ceph fs status <fs-name>
ceph fs get <fs-name>

# åˆ é™¤æ–‡ä»¶ç³»ç»Ÿ
ceph fs fail <fs-name>
ceph fs rm <fs-name> --yes-i-really-mean-it

# MDS ç®¡ç†
ceph mds stat
ceph mds dump
ceph mds fail <mds-id>
ceph mds repaired <mds-id>
```

### 10.2 CephFS å®¢æˆ·ç«¯
```bash
# æŒ‚è½½ CephFS
mount -t ceph <mon-addr>:/ /mnt/cephfs -o name=admin,secret=<key>
mount -t ceph <mon-addr>:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret

# ä½¿ç”¨ ceph-fuse
ceph-fuse /mnt/cephfs
ceph-fuse /mnt/cephfs -o allow_other

# æŸ¥çœ‹å®¢æˆ·ç«¯è¿æ¥
ceph daemon mds.<id> client ls
ceph daemon mds.<id> session ls

# è¸¢é™¤å®¢æˆ·ç«¯
ceph daemon mds.<id> client evict id=<client-id>
```

### 10.3 CephFS é«˜çº§ç®¡ç†
```bash
# å­ç›®å½•æŒ‚è½½
mount -t ceph <mon-addr>:/subdir /mnt/subdir -o name=admin

# é…ç½®å¤šä¸ªæ–‡ä»¶ç³»ç»Ÿ
ceph fs flag set enable_multiple true

# ç›®å½•é…é¢
setfattr -n ceph.quota.max_files -v 10000 /mnt/cephfs/dir
setfattr -n ceph.quota.max_bytes -v 1000000000 /mnt/cephfs/dir
getfattr -n ceph.quota.max_files /mnt/cephfs/dir
```

---

## â˜ï¸ 11. RGW ç®¡ç†

### 11.1 RGW åŸºæœ¬æ“ä½œ
```bash
# åˆ›å»ºç”¨æˆ·
radosgw-admin user create --uid=<user-id> --display-name="<display-name>"
radosgw-admin user create --uid=testuser --display-name="Test User" --email=test@example.com

# æŸ¥çœ‹ç”¨æˆ·
radosgw-admin user list
radosgw-admin user info --uid=<user-id>

# åˆ é™¤ç”¨æˆ·
radosgw-admin user rm --uid=<user-id>

# ä¿®æ”¹ç”¨æˆ·
radosgw-admin user modify --uid=<user-id> --display-name="New Name"

# åˆ›å»ºå­ç”¨æˆ·
radosgw-admin subuser create --uid=<user-id> --subuser=<subuser-id> --access=full
```

### 11.2 RGW å¯†é’¥ç®¡ç†
```bash
# åˆ›å»ºè®¿é—®å¯†é’¥
radosgw-admin key create --uid=<user-id> --key-type=s3

# åˆ é™¤è®¿é—®å¯†é’¥
radosgw-admin key rm --uid=<user-id> --key-type=s3 --access-key=<access-key>

# ç”ŸæˆSwiftå¯†é’¥
radosgw-admin key create --uid=<user-id> --key-type=swift --gen-secret
```

### 11.3 RGW ç»´æŠ¤
```bash
# æ£€æŸ¥ bucket ç´¢å¼•
radosgw-admin bucket check --bucket=<bucket-name>

# é‡å»º bucket ç´¢å¼•
radosgw-admin bucket reshard --bucket=<bucket-name> --num-shards=<num>

# åƒåœ¾å›æ”¶
radosgw-admin gc list
radosgw-admin gc process

# æŸ¥çœ‹ä½¿ç”¨ç»Ÿè®¡
radosgw-admin usage show --uid=<user-id>
radosgw-admin usage show --show-log-entries=false

# Bucket ç®¡ç†
radosgw-admin bucket list
radosgw-admin bucket stats --bucket=<bucket-name>
```

---

## ğŸ”§ 12. é…ç½®ç®¡ç†

### 12.1 è¿è¡Œæ—¶é…ç½®
```bash
# æŸ¥çœ‹é…ç½®
ceph config dump
ceph config get <daemon-type>.<daemon-id> <option>
ceph config show <daemon-type>.<daemon-id>

# è®¾ç½®é…ç½®
ceph config set <daemon-type> <option> <value>
ceph config set global osd_pool_default_size 3
ceph config set osd osd_max_backfills 1

# é‡ç½®é…ç½®
ceph config rm <daemon-type> <option>

# æŸ¥çœ‹é…ç½®å†å²
ceph config log
```

### 12.2 ä¸´æ—¶é…ç½®è°ƒæ•´
```bash
# ä¸´æ—¶è®¾ç½®é…ç½®ï¼ˆé‡å¯åå¤±æ•ˆï¼‰
ceph tell <daemon-type>.<id> config set <option> <value>
ceph tell osd.* config set debug_osd 10

# æŸ¥çœ‹é…ç½®
ceph tell <daemon-type>.<id> config show
ceph tell <daemon-type>.<id> config get <option>

# æ‰¹é‡é…ç½®
ceph tell osd.* config set osd_max_backfills 1
ceph tell mon.* config set debug_mon 5
```

---

## ğŸ“ˆ 13. æ€§èƒ½åˆ†æä¸“é¡¹

### 13.1 RBD æ€§èƒ½åˆ†æ

#### RBD æ€§èƒ½æµ‹è¯•å·¥å…·
```bash
# ä½¿ç”¨ rbd bench è¿›è¡Œæ€§èƒ½æµ‹è¯•
rbd bench --io-type write --io-size 4K --io-threads 16 --io-total 1G <pool>/<image>
rbd bench --io-type read --io-size 4K --io-threads 16 --io-total 1G <pool>/<image>
rbd bench --io-type readwrite --io-size 4K --io-threads 16 --io-total 1G <pool>/<image>

# ä½¿ç”¨ fio è¿›è¡Œè¯¦ç»†æ€§èƒ½æµ‹è¯•
fio --name=rbd-test --ioengine=rbd --pool=<pool> --rbdname=<image> \
    --rw=randwrite --bs=4k --iodepth=32 --numjobs=4 --runtime=60 --group_reporting

# æµ‹è¯•ä¸åŒå—å¤§å°çš„æ€§èƒ½
for bs in 4k 8k 16k 32k 64k 128k; do
    echo "Testing block size: $bs"
    rbd bench --io-type write --io-size $bs --io-threads 16 --io-total 512M <pool>/<image>
done

# é¡ºåºè¯»å†™æµ‹è¯•
fio --name=seq-write --ioengine=rbd --pool=test --rbdname=testimg \
    --rw=write --bs=1M --iodepth=1 --numjobs=1 --runtime=60
```

#### RBD æ€§èƒ½ç›‘æ§
```bash
# å®æ—¶ç›‘æ§ RBD I/O
rbd perf image iostat

# æŸ¥çœ‹ RBD é•œåƒç»Ÿè®¡
rbd du <pool>/<image>
rbd info <pool>/<image>

# ç›‘æ§å®¢æˆ·ç«¯ I/O æ¨¡å¼
ceph daemon client.<id> perf dump
ceph daemon client.<id> perf histogram dump

# æŸ¥çœ‹ RBD ç¼“å­˜ç»Ÿè®¡
ceph daemon client.<id> config show | grep rbd_cache
ceph daemon client.<id> perf dump | grep rbd_cache
```

#### RBD æ€§èƒ½è°ƒä¼˜å‚æ•°
```bash
# å®¢æˆ·ç«¯ç¼“å­˜ä¼˜åŒ–
ceph config set client rbd_cache true
ceph config set client rbd_cache_size 134217728           # 128MB
ceph config set client rbd_cache_max_dirty 100663296      # 96MB
ceph config set client rbd_cache_target_dirty 67108864    # 64MB
ceph config set client rbd_cache_max_dirty_age 10.0

# é¢„è¯»ä¼˜åŒ–
ceph config set client rbd_readahead_trigger_requests 10
ceph config set client rbd_readahead_max_bytes 524288     # 512KB
ceph config set client rbd_readahead_disable_after_bytes 52428800  # 50MB

# å¹¶å‘ä¼˜åŒ–
ceph config set client rbd_concurrent_management_ops 20
ceph config set client rbd_op_threads 1
```

### 13.2 CephFS æ€§èƒ½åˆ†æ

#### CephFS æ€§èƒ½æµ‹è¯•
```bash
# ä½¿ç”¨ dd è¿›è¡ŒåŸºæœ¬æ€§èƒ½æµ‹è¯•
dd if=/dev/zero of=/mnt/cephfs/testfile bs=1M count=1000 oflag=direct
dd if=/mnt/cephfs/testfile of=/dev/null bs=1M iflag=direct

# ä½¿ç”¨ iozone è¿›è¡Œå…¨é¢æ€§èƒ½æµ‹è¯•
iozone -a -s 1G -r 4k -r 64k -r 1M -i 0 -i 1 -i 2 -f /mnt/cephfs/testfile

# å°æ–‡ä»¶æ€§èƒ½æµ‹è¯•
mkdir /mnt/cephfs/small_files_test
time for i in {1..10000}; do touch /mnt/cephfs/small_files_test/file_$i; done

# å…ƒæ•°æ®æ€§èƒ½æµ‹è¯•
time find /mnt/cephfs -name "*.txt" | wc -l
time ls -la /mnt/cephfs/large_directory/ | wc -l

# å¹¶å‘æ–‡ä»¶æ“ä½œæµ‹è¯•
for i in {1..10}; do
    (for j in {1..1000}; do
        touch /mnt/cephfs/test_${i}_${j}
    done) &
done
wait
```

#### CephFS æ€§èƒ½ç›‘æ§
```bash
# MDS æ€§èƒ½ç»Ÿè®¡
ceph daemon mds.<id> perf dump
ceph daemon mds.<id> session ls
ceph daemon mds.<id> status
ceph daemon mds.<id> ops

# å®¢æˆ·ç«¯æ€§èƒ½ç»Ÿè®¡
ceph daemon client.<id> perf dump
ceph daemon client.<id> client_metadata

# æŸ¥çœ‹ MDS è´Ÿè½½
ceph fs status <fsname>
ceph mds metadata <mds-id>

# ç›‘æ§ç›®å½•ç¢ç‰‡åŒ–
ceph daemon mds.<id> dirfrag ls <path>
ceph daemon mds.<id> dirfrag split_info <path>
```



### 13.3 OSD æ€§èƒ½åˆ†æ

#### OSD æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# OSD åŸºå‡†æµ‹è¯•
ceph tell osd.<id> bench 1073741824 4194304              # 1GBæµ‹è¯•ï¼Œ4MBå—å¤§å°

# ä½¿ç”¨ ceph-osd ç›´æ¥æµ‹è¯•
ceph-osd -i <id> --test-objectstore --test-objectstore-workload uniform_random

# ç£ç›˜æ€§èƒ½æµ‹è¯•
dd if=/dev/zero of=/var/lib/ceph/osd/ceph-<id>/test bs=1M count=1000 oflag=direct
dd if=/var/lib/ceph/osd/ceph-<id>/test of=/dev/null bs=1M iflag=direct

# ç½‘ç»œæ€§èƒ½æµ‹è¯•
iperf3 -s    # åœ¨ç›®æ ‡OSDèŠ‚ç‚¹è¿è¡Œ
iperf3 -c <target-osd-ip> -t 30 -P 4    # åœ¨æºèŠ‚ç‚¹è¿è¡Œ

# ä½¿ç”¨ rados bench æµ‹è¯•
rados bench -p <pool> 60 write --no-cleanup
rados bench -p <pool> 60 seq
rados bench -p <pool> 60 rand
rados -p <pool> cleanup
```

#### OSD è¯¦ç»†æ€§èƒ½ç›‘æ§
```bash
# OSD æ€§èƒ½è®¡æ•°å™¨
ceph daemon osd.<id> perf dump
ceph daemon osd.<id> perf histogram dump

# OSD æ“ä½œç»Ÿè®¡
ceph daemon osd.<id> dump_ops_in_flight
ceph daemon osd.<id> dump_blocked_ops
ceph daemon osd.<id> dump_op_pq_state

# OSD å­˜å‚¨ç»Ÿè®¡
ceph daemon osd.<id> status
ceph daemon osd.<id> df

# æ…¢æ“ä½œåˆ†æ
ceph daemon osd.<id> dump_slow_requests
ceph daemon osd.<id> dump_historic_slow_ops

# OSD æ¢å¤ç»Ÿè®¡
ceph daemon osd.<id> dump_recovery_reservations
ceph daemon osd.<id> dump_backfill_reservations
```


### 13.4 æ•´ä½“é›†ç¾¤æ€§èƒ½åˆ†æ

#### é›†ç¾¤çº§æ€§èƒ½ç›‘æ§
```bash
# é›†ç¾¤ I/O ç»Ÿè®¡
ceph iostat
watch "ceph iostat"

# PG åˆ†å¸ƒåˆ†æ
ceph pg dump | grep ^pg | awk '{print $1,$15}' | sort

# å®æ—¶æ€§èƒ½ç›‘æ§è„šæœ¬
#!/bin/bash
while true; do
    echo "$(date) - IOPS: $(ceph iostat | grep -E 'read|write')"
    echo "$(date) - Health: $(ceph health)"
    sleep 5
done

# é›†ç¾¤å»¶è¿Ÿåˆ†æ
ceph osd perf | grep -E "apply_latency|commit_latency"


```

---

## ğŸ› ï¸ 14. Ceph ä¸“ç”¨å·¥å…·é›†

### 14.1 æ•°æ®æ¢å¤å’Œè¯Šæ–­å·¥å…·

#### ceph-objectstore-tool (å¯¹è±¡å­˜å‚¨å·¥å…·)
```bash
# åˆ—å‡ºå¯¹è±¡å­˜å‚¨å†…å®¹
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> --op list

# å¯¼å‡º PG æ•°æ®
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op export --pgid <pg-id> --file /tmp/pg_export.dat

# å¯¼å…¥ PG æ•°æ®
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op import --file /tmp/pg_export.dat

# åˆ é™¤æŸåçš„å¯¹è±¡
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op remove --pgid <pg-id> --oid <object-id>

# ä¿®å¤å¯¹è±¡å­˜å‚¨
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op fsck --debug

# æŸ¥çœ‹å¯¹è±¡ä¿¡æ¯
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op info --pgid <pg-id> --oid <object-id>
```

#### ceph-kvstore-tool (é”®å€¼å­˜å‚¨å·¥å…·)
```bash
# åˆ—å‡ºé”®å€¼å¯¹
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> list

# è·å–ç‰¹å®šé”®å€¼
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> get <prefix> <key>

# åˆ é™¤é”®å€¼å¯¹
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> rm <prefix> <key>

# å‹ç¼©æ•°æ®åº“
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> compact

# å¤‡ä»½/æ¢å¤é”®å€¼å­˜å‚¨
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> backup /tmp/kv_backup
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> restore /tmp/kv_backup

# ç»Ÿè®¡ä¿¡æ¯
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> stats
```

#### ceph-bluestore-tool (BlueStore ä¸“ç”¨å·¥å…·)
```bash
# æŸ¥çœ‹ BlueStore ä¿¡æ¯
ceph-bluestore-tool show-label --dev /dev/sdX
ceph-bluestore-tool prime-osd-dir --dev /dev/sdX --path /var/lib/ceph/osd/ceph-X

# ä¿®å¤ BlueStore
ceph-bluestore-tool fsck --path /var/lib/ceph/osd/ceph-<id>
ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-<id>

# å±•ç¤º BlueStore å…ƒæ•°æ®
ceph-bluestore-tool show-label --dev /dev/sdX
ceph-bluestore-tool show-label --path /var/lib/ceph/osd/ceph-<id>

# è“å­˜å‚¨ç©ºé—´åˆ†æ
ceph-bluestore-tool bluefs-stats --path /var/lib/ceph/osd/ceph-<id>
ceph-bluestore-tool bluefs-bdev-sizes --path /var/lib/ceph/osd/ceph-<id>

# åˆ†é…å™¨å·¥å…·
ceph-bluestore-tool free-dump --path /var/lib/ceph/osd/ceph-<id>
ceph-bluestore-tool free-score --path /var/lib/ceph/osd/ceph-<id>
```

### 14.2 ç›‘æ§å’Œç®¡ç†å·¥å…·

#### crushtool (CRUSH æ˜ å°„å·¥å…·)
```bash
# ç¼–è¯‘/åç¼–è¯‘ CRUSH map
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt
crushtool -c crushmap.txt -o crushmap_new.bin
ceph osd setcrushmap -i crushmap_new.bin

# æµ‹è¯• CRUSH è§„åˆ™
crushtool -i crushmap.bin --test --show-mappings --ruleset 0 --num-rep 3 --min-x 0 --max-x 100

# åˆ†æ CRUSH æƒé‡
crushtool -i crushmap.bin --show-choose-tries

# éªŒè¯ CRUSH map
crushtool -i crushmap.bin --check

# æ˜¾ç¤º CRUSH æ ‘ç»“æ„
crushtool -i crushmap.bin --tree
```

#### monmaptool (Monitor æ˜ å°„å·¥å…·)
```bash
# åˆ›å»º Monitor map
monmaptool --create --add <mon-id> <mon-addr> --fsid <uuid> monmap.bin

# æŸ¥çœ‹ Monitor map
monmaptool --print monmap.bin

# æ·»åŠ /åˆ é™¤ Monitor
monmaptool --add <mon-id> <mon-addr> monmap.bin
monmaptool --rm <mon-id> monmap.bin

# ç”Ÿæˆæ–°çš„ fsid
uuidgen
monmaptool --create --fsid $(uuidgen) monmap.bin

# ä»ç°æœ‰é›†ç¾¤æå–
ceph mon getmap -o monmap.bin
monmaptool --print monmap.bin
```

#### osdmaptool (OSD æ˜ å°„å·¥å…·)
```bash
# å¯¼å‡º OSD map
ceph osd getmap -o osdmap.bin
osdmaptool osdmap.bin --print

# æµ‹è¯• PG æ˜ å°„
osdmaptool osdmap.bin --test-map-pgs --pool <pool-id>

# åˆ›å»ºå¢é‡ map
osdmaptool --createsimple 3 osdmap.bin --with-default-pool

# å¯¼å‡ºç‰¹å®š epoch çš„ map
ceph osd getmap <epoch> -o osdmap_<epoch>.bin

# æ˜¾ç¤º OSD æ ‘
osdmaptool osdmap.bin --tree
```

### 14.3 æ•°æ®è¿ç§»å’ŒåŒæ­¥å·¥å…·

#### rados (å¯¹è±¡å­˜å‚¨å®¢æˆ·ç«¯)
```bash
# åŸºæœ¬å¯¹è±¡æ“ä½œ
rados -p <pool> put <object> <file>
rados -p <pool> get <object> <file>
rados -p <pool> ls
rados -p <pool> rm <object>

# æ‰¹é‡æ“ä½œ
rados -p <pool> ls | xargs -I {} rados -p <pool> rm {}

# åŸºå‡†æµ‹è¯•
rados bench -p <pool> 60 write --no-cleanup
rados bench -p <pool> 60 seq
rados bench -p <pool> 60 rand

# æ¸…ç†åŸºå‡†æµ‹è¯•æ•°æ®
rados -p <pool> cleanup

# å¯¹è±¡å±æ€§æ“ä½œ
rados -p <pool> setxattr <object> <attr> <value>
rados -p <pool> getxattr <object> <attr>
rados -p <pool> listxattr <object>

# ç›‘è§†å¯¹è±¡å˜åŒ–
rados -p <pool> watch <object>
```

#### rbd-mirror (RBD é•œåƒå·¥å…·)
```bash
# å¯ç”¨é•œåƒ
rbd mirror pool enable <pool> pool
rbd mirror pool enable <pool> image

# é…ç½®é•œåƒ
rbd mirror image enable <pool>/<image> snapshot
rbd mirror image enable <pool>/<image> journal

# æŸ¥çœ‹é•œåƒçŠ¶æ€
rbd mirror pool status <pool>
rbd mirror image status <pool>/<image>

# å¼ºåˆ¶é‡æ–°åŒæ­¥
rbd mirror image resync <pool>/<image>

# æ•…éšœè½¬ç§»
rbd mirror image promote <pool>/<image>
rbd mirror image demote <pool>/<image>
```



#### ceph-crash (å´©æºƒæŠ¥å‘Šå·¥å…·)
```bash
# æŸ¥çœ‹å´©æºƒæŠ¥å‘Š
ceph crash ls
ceph crash info <crash-id>

# å½’æ¡£å´©æºƒæŠ¥å‘Š
ceph crash archive <crash-id>
ceph crash archive-all

# ç”Ÿæˆå´©æºƒæŠ¥å‘Š
ceph crash post -i <crash-dump-file>

# ç»Ÿè®¡å´©æºƒæŠ¥å‘Š
ceph crash stat
```

#### ceph-volume (å·ç®¡ç†å·¥å…·)
```bash
# LVM æ–¹å¼åˆ›å»º OSD
ceph-volume lvm create --data /dev/sdb
ceph-volume lvm create --data /dev/sdb --block.wal /dev/nvme0n1p1

# å‡†å¤‡å’Œæ¿€æ´»åˆ†ç¦»
ceph-volume lvm prepare --data /dev/sdb --osd-id 0
ceph-volume lvm activate 0 <osd-uuid>

# æŸ¥çœ‹ LVM ä¿¡æ¯
ceph-volume lvm list
ceph-volume lvm list /dev/sdb

# ç®€å•æ–¹å¼åˆ›å»º OSD
ceph-volume simple scan /dev/sdb
ceph-volume simple activate <osd-id> <osd-uuid>
```

---

## ğŸš¨ 15. æ•…éšœæ’æŸ¥

### 15.1 æ—¥å¿—æŸ¥çœ‹
```bash
# æŸ¥çœ‹å„æœåŠ¡æ—¥å¿—
journalctl -u ceph-mon@<hostname> -f
journalctl -u ceph-osd@<id> -f
journalctl -u ceph-mds@<hostname> -f
journalctl -u ceph-mgr@<hostname> -f

# æŒ‰æ—¶é—´èŒƒå›´æŸ¥çœ‹æ—¥å¿—
journalctl -u ceph-osd@<id> --since "2023-01-01 00:00:00" --until "2023-01-01 23:59:59"

# è°ƒæ•´æ—¥å¿—çº§åˆ«
ceph tell osd.* config set debug_osd 10
ceph tell mon.* config set debug_mon 10
ceph tell mds.* config set debug_mds 10

# æŸ¥çœ‹å®æ—¶é”™è¯¯
tail -f /var/log/ceph/ceph.log | grep -i error
tail -f /var/log/ceph/ceph-osd.*.log | grep -i slow
```



### 15.2 å¸¸è§é—®é¢˜æ’æŸ¥
```bash
# PG ä¸ä¸€è‡´é—®é¢˜
ceph health detail | grep inconsistent
ceph pg <pgid> query
ceph pg repair <pgid>

# OSD æ»¡è½½é—®é¢˜
ceph osd df | grep -E "(95|96|97|98|99)%"
ceph osd reweight-by-utilization

# æ…¢è¯·æ±‚é—®é¢˜
ceph daemon osd.<id> dump_slow_requests
ceph daemon osd.<id> dump_historic_slow_ops
ceph config set osd debug_osd 10

# æ—¶é’ŸåŒæ­¥é—®é¢˜
ceph time-sync-status
ntpq -p

# ç£ç›˜ç©ºé—´é—®é¢˜
ceph df
ceph osd df
df -h /var/lib/ceph/osd/
```

---

## ğŸ”„ 16. å¤‡ä»½æ¢å¤

### 16.1 æ•°æ®å¤‡ä»½
```bash
# å¯¼å‡º Monitor map
ceph mon getmap -o monmap.bin

# å¯¼å‡º OSD map
ceph osd getmap -o osdmap.bin

# å¯¼å‡º CRUSH map
ceph osd getcrushmap -o crushmap.bin

# å¤‡ä»½ ceph.conf å’Œå¯†é’¥
cp /etc/ceph/ceph.conf /backup/
cp /etc/ceph/ceph.client.admin.keyring /backup/

# å¤‡ä»½è®¤è¯ä¿¡æ¯
ceph auth export > /backup/ceph_auth.txt

# å¤‡ä»½å­˜å‚¨æ± ä¿¡æ¯
ceph osd pool ls detail > /backup/pools.txt
ceph pg dump > /backup/pg_dump.txt
```

### 16.2 ç¾éš¾æ¢å¤
```bash
# ä»å•ä¸ª OSD æ¢å¤æ•°æ®
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-X \
    --export-remove --pgid <pgid> --file /tmp/pg_export

# é‡å»º Monitor
ceph-mon --mkfs -i <mon-id> --monmap monmap.bin --keyring mon.keyring

# æ¢å¤è®¤è¯ä¿¡æ¯
ceph auth import -i /backup/ceph_auth.txt

# æ¢å¤ CRUSH map
ceph osd setcrushmap -i /backup/crushmap.bin

# OSD æ•°æ®æ¢å¤
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-X \
    --op import --file /tmp/pg_export
```

### 16.3 RBD å¤‡ä»½æ¢å¤
```bash
# RBD å¿«ç…§å¤‡ä»½
rbd snap create <pool>/<image>@backup-$(date +%Y%m%d)
rbd export <pool>/<image>@<snapshot> /backup/image_backup.raw

# RBD å¢é‡å¤‡ä»½
rbd export-diff <pool>/<image>@<snap1> <pool>/<image>@<snap2> /backup/diff.raw

# RBD æ¢å¤
rbd import /backup/image_backup.raw <pool>/<new-image>
rbd import-diff /backup/diff.raw <pool>/<image>
```


**æœ¬æ–‡æ¡£æŒç»­æ›´æ–°ï¼Œå»ºè®®å®šæœŸæ£€æŸ¥æœ€æ–°ç‰ˆæœ¬ã€‚å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿åé¦ˆã€‚**

*æœ€åæ›´æ–°æ—¶é—´ï¼š2025å¹´06æœˆ*
