# Practical Guide: Ceph Command Tools Summary
## üìã Common Tools (Summary Overview)
| Function Category | Main Commands | Verification Status | Usage Frequency | Application Scenarios | Risk Level |
|---------|---------|---------|---------|---------|---------|
| **Cluster Monitoring** | `ceph -s`, `ceph health(detail)`, `ceph df`, `ceph -w`| ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Daily monitoring, troubleshooting | üü¢ No risk |
| **I/O Monitoring** | `ceph iostat`(version dependent,N+), `ceph -w`, `ceph status` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê | Performance monitoring | üü¢ No risk |
| **OSD Management** | `ceph osd tree`, `ceph osd status`, `ceph osd out/in` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê | OSD maintenance, capacity management | üü° Medium risk (queries safe) |
| **Monitor Management** | `ceph mon stat`, `ceph quorum_status`, `ceph mon add/remove` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê | Cluster management, high availability | üî¥ High risk (queries safe)|
| **Manager Management** | `ceph mgr module enable/disable`, `ceph mgr stat` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê | Feature management, dashboard | üü° Medium risk (queries safe) |
| **Pool Management** | `ceph osd pool create/delete`, `ceph osd pool set` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê | Storage planning, quota management | üî¥ High risk (queries safe) |
| **PG Management** | `ceph pg stat`, `ceph pg repair`, `ceph pg scrub` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê | Data integrity, fault repair | üü° Medium risk (queries safe) |
| **Authentication Management** | `ceph auth list/create/del` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê | Security management, access control | üî¥ High risk (queries safe) |
| **CRUSH Management** | `ceph osd crush tree`, `crushtool`, `ceph osd crush rule` | ‚úÖ Verified | ‚≠ê‚≠ê | Data distribution, failure domains | üî¥ High risk (queries safe) |
| **RBD Management** | `rbd create/rm`, `rbd snap create`, `rbd map/unmap` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê | Block storage, snapshot management | üü° Medium risk |
| **CephFS Management** | `ceph fs status`, `ceph mds stat`, `ceph fs dump`, `ceph mds fail` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê | File system, metadata | üü° Medium risk (queries safe) |
| **RGW Management** | `radosgw-admin user create`, `radosgw-admin bucket` | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê | Object storage, user management | üü° Medium risk (queries safe)|
| **Configuration Management** | `ceph config set/get`, `ceph tell` | Not verified | ‚≠ê‚≠ê‚≠ê‚≠ê | Parameter tuning, fault handling | üü° Medium risk (queries safe) |
| **Performance Analysis** | `ceph osd perf`,`rbd perf image iostat`, cephfs-top | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê | Performance testing, bottleneck analysis | üü¢ No risk |
| **Specialized Tools** | `ceph-objectstore-tool`, `ceph-bluestore-tool` | ‚úÖ Verified | ‚≠ê‚≠ê | Data recovery, deep diagnostics | üî¥ High risk (queries safe) |
| **Troubleshooting** | `journalctl`, `ceph daemon dump`, log analysis | ‚úÖ Verified | ‚≠ê‚≠ê‚≠ê‚≠ê | Problem diagnosis, root cause analysis | üü¢ No risk |
| **Backup Recovery** | `ceph mon getmap`, `ceph auth export`, data export | ‚úÖ Verified | ‚≠ê‚≠ê | Disaster recovery, migration | üü° Medium risk (queries safe) |
## üîß 1. Cluster Status Monitoring
### 1.1 Overall Cluster Status
```bash
# View cluster status (most commonly used)
ceph status
ceph -s
# View cluster health status
ceph health
ceph health detail
# Real-time cluster monitoring
ceph -w
# View cluster storage usage
ceph df
ceph df detail
# View cluster version information
ceph version
ceph versions
```
### 1.2 Cluster Performance Monitoring
```bash
# View cluster I/O statistics
ceph iostat
watch "ceph iostat"
# View OSD performance statistics
ceph osd perf
# View slow requests
ceph daemon osd.X dump_slow_requests
ceph daemon osd.X dump_historic_slow_ops
# PG distribution analysis
ceph pg dump | grep ^pg | awk '{print $1,$15}' | sort
```
---
## üóÑÔ∏è 2. OSD Management
### 2.1 Basic OSD Operations
```bash
# View OSD status
ceph osd stat
ceph osd dump
ceph osd tree
# View OSD usage
ceph osd df
ceph osd df tree
# Start/stop OSD
systemctl start ceph-osd@X
systemctl stop ceph-osd@X
systemctl restart ceph-osd@X
# Mark OSD as out/in
ceph osd out X
ceph osd in X
# Mark OSD as down/up
ceph osd down X
ceph osd up X
```
### 2.2 OSD Maintenance Operations
```bash
# Safely remove OSD (complete workflow)
ceph osd out X                              # Mark as out, start data migration
ceph osd safe-to-destroy X                  # Check if safe to remove
ceph osd destroy X --yes-i-really-mean-it   # Destroy OSD
ceph osd crush remove osd.X                 # Remove from CRUSH map
ceph auth del osd.X                         # Delete authentication info
ceph osd rm X                               # Remove from cluster
# Replace failed OSD
ceph osd destroy X --yes-i-really-mean-it
# Reweight OSD
ceph osd reweight X 0.8                     # Temporary weight adjustment
ceph osd crush reweight osd.X 2.0           # Permanent weight adjustment
ceph osd reweight-by-utilization            # Auto adjust weight by utilization
```
### 2.3 OSD Troubleshooting
```bash
# View detailed OSD information
ceph osd find X
ceph osd metadata X
# View OSD logs
journalctl -u ceph-osd@X -f
journalctl -u ceph-osd@X --since "1 hour ago"
# Repair OSD
ceph osd scrub X
ceph osd deep-scrub X
# View PGs on OSD
ceph pg ls-by-osd X
# OSD performance diagnostics
ceph daemon osd.X perf dump
ceph daemon osd.X dump_ops_in_flight
ceph daemon osd.X dump_blocked_ops
```
---
## üèõÔ∏è 3. Monitor Management
### 3.1 Basic Monitor Operations
```bash
# View Monitor status
ceph mon stat
ceph mon dump
# View Monitor quorum status
ceph quorum_status
# Add Monitor
ceph mon add <mon-id> <mon-addr>
# Remove Monitor
ceph mon remove <mon-id>
# View Monitor map
ceph mon getmap -o /tmp/monmap
monmaptool --print /tmp/monmap
```
### 3.2 Monitor Maintenance
```bash
# Start/stop Monitor
systemctl start ceph-mon@<hostname>
systemctl stop ceph-mon@<hostname>
# Compact Monitor database
ceph tell mon.<mon-id> compact
# Monitor time synchronization
ceph time-sync-status
# Monitor failure recovery
ceph-mon --extract-monmap /tmp/monmap --mon-data /var/lib/ceph/mon/ceph-<id>
```
---
## üë®‚Äçüíº 4. Manager Management
### 4.1 Basic Manager Operations
```bash
# View Manager status
ceph mgr stat
ceph mgr dump
# Enable/disable Manager modules
ceph mgr module enable <module>
ceph mgr module disable <module>
# View available modules
ceph mgr module ls
# Failover Manager
ceph mgr fail <mgr-id>
```
### 4.2 Common Manager Modules
```bash
# Dashboard module
ceph mgr module enable dashboard
ceph dashboard create-self-signed-cert
ceph dashboard ac-user-create <username> -i <password-file>
ceph dashboard ac-role-add-scope-perms admin read-write pool
ceph config set mgr mgr/dashboard/server_addr 0.0.0.0
ceph config set mgr mgr/dashboard/server_port 8443
# Prometheus module
ceph mgr module enable prometheus
ceph config set mgr mgr/prometheus/server_addr 0.0.0.0
ceph config set mgr mgr/prometheus/server_port 9283
# Balancer module
ceph mgr module enable balancer
ceph balancer mode upmap
ceph balancer on
ceph balancer status
# Alerts module
ceph mgr module enable alerts
```
---
## üóÇÔ∏è 5. Pool Management
### 5.1 Basic Pool Operations
```bash
# Create pool
ceph osd pool create <poolname> <pg_num> [pgp_num]
ceph osd pool create rbd 128 128 replicated
ceph osd pool create mypool 64 64 erasure
# Delete pool
ceph config set mon mon_allow_pool_delete true
ceph osd pool delete <poolname> <poolname> --yes-i-really-really-mean-it
# View pools
ceph osd lspools
ceph osd pool ls detail
# Set pool quota
ceph osd pool set-quota <poolname> max_objects 1000
ceph osd pool set-quota <poolname> max_bytes 1TB
# View pool statistics
ceph osd pool stats
ceph osd pool stats <poolname>
```
### 5.2 Pool Parameter Configuration
```bash
# Set pool replica count
ceph osd pool set <poolname> size 3
ceph osd pool set <poolname> min_size 2
# Set PG count
ceph osd pool set <poolname> pg_num 128
ceph osd pool set <poolname> pgp_num 128
# Set pool type
ceph osd pool set <poolname> crush_rule <rule-name>
# Enable/disable features
ceph osd pool set <poolname> noscrub true
ceph osd pool set <poolname> nodeep-scrub true
# Pool application tags
ceph osd pool application enable <poolname> rbd
ceph osd pool application enable <poolname> cephfs
ceph osd pool application enable <poolname> rgw
```
---
## üìÑ 6. Placement Group (PG) Management
### 6.1 PG Status Viewing
```bash
# View PG status
ceph pg stat
ceph pg dump
ceph pg dump --format json-pretty
# View abnormal PGs
ceph pg dump_stuck
ceph pg dump_stuck inactive
ceph pg dump_stuck unclean
# View specific PG information
ceph pg <pgid> query
ceph pg map <pgid>
```
### 6.2 PG Repair Operations
```bash
# Repair PG
ceph pg repair <pgid>
ceph pg scrub <pgid>
ceph pg deep-scrub <pgid>
# Force repair inconsistent objects
ceph pg force-recovery <pgid>
ceph pg force-backfill <pgid>
# View PG's OSDs
ceph pg ls-by-pool <poolname>
ceph pg ls-by-primary <osd-id>
# PG auto repair
ceph config set osd osd_scrub_auto_repair true
```
---
## üîê 7. Authentication Management
### 7.1 User Management
```bash
# View authentication information
ceph auth list
ceph auth get client.admin
# Create user
ceph auth get-or-create client.<name> mon 'allow r' osd 'allow rw pool=<poolname>'
ceph auth get-or-create client.rbd mon 'profile rbd' osd 'profile rbd pool=rbd'
# Delete user
ceph auth del client.<name>
# Export/import keys
ceph auth export client.<name> > client.<name>.keyring
ceph auth import -i client.<name>.keyring
```
### 7.2 Permission Management
```bash
# Modify user permissions
ceph auth caps client.<name> mon 'allow r' osd 'allow rw pool=rbd'
# View permissions
ceph auth get client.<name>
# Generate client configuration
ceph config generate-minimal-conf
# Common permission templates
# Read-only user: mon 'allow r' osd 'allow r'
# RBD user: mon 'profile rbd' osd 'profile rbd pool=rbd'
# CephFS user: mon 'allow r' mds 'allow rw' osd 'allow rw pool=cephfs_data'
```
---
## üèóÔ∏è 8. CRUSH Map Management
### 8.1 Basic CRUSH Operations
```bash
# View CRUSH map
ceph osd tree
ceph osd crush tree
# Export/import CRUSH map
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt
crushtool -c crushmap.txt -o crushmap-new.bin
ceph osd setcrushmap -i crushmap-new.bin
# Create/delete bucket
ceph osd crush add-bucket <bucket-name> <bucket-type>
ceph osd crush remove <bucket-name>
# Move OSD to different location
ceph osd crush move osd.X host=<hostname>
ceph osd crush move osd.X rack=<rackname>
```
### 8.2 CRUSH Rule Management
```bash
# View CRUSH rules
ceph osd crush rule ls
ceph osd crush rule dump
# Create CRUSH rule
ceph osd crush rule create-simple <rule-name> <root> <type>
ceph osd crush rule create-replicated <rule-name> <root> <failure-domain> <device-class>
# Delete CRUSH rule
ceph osd crush rule rm <rule-name>
# Test CRUSH mapping
crushtool -i crushmap.bin --test --show-mappings --ruleset 0 --num-rep 3 --min-x 0 --max-x 100
```
---
## üéØ 9. RBD Management
### 9.1 Basic RBD Operations
```bash
# Create RBD image
rbd create --size 1024 <poolname>/<image-name>
rbd create --size 10G --image-feature layering,exclusive-lock mypool/myimage
# View RBD images
rbd ls <poolname>
rbd info <poolname>/<image-name>
rbd du <poolname>/<image-name>
# Delete RBD image
rbd rm <poolname>/<image-name>
rbd trash mv <poolname>/<image-name>    # Move to trash
rbd trash restore <poolname>/<image-id>  # Restore from trash
# Resize image
rbd resize --size 2048 <poolname>/<image-name>
rbd resize --size 2G <poolname>/<image-name>
# Create snapshot
rbd snap create <poolname>/<image-name>@<snap-name>
rbd snap ls <poolname>/<image-name>
rbd snap rm <poolname>/<image-name>@<snap-name>
```
### 9.2 Advanced RBD Operations
```bash
# Clone image
rbd snap protect <poolname>/<parent-image>@<snap-name>
rbd clone <poolname>/<parent-image>@<snap-name> <poolname>/<child-image>
# Flatten cloned image
rbd flatten <poolname>/<image-name>
# Export/import image
rbd export <poolname>/<image-name> image.raw
rbd export --export-format 2 <poolname>/<image-name> image.raw
rbd import image.raw <poolname>/<image-name>
# Image mapping
rbd map <poolname>/<image-name>
rbd unmap /dev/rbdX
rbd showmapped
# Image feature management
rbd feature enable <poolname>/<image-name> exclusive-lock
rbd feature disable <poolname>/<image-name> deep-flatten
```
---
## üåê 10. CephFS Management
### 10.1 Basic CephFS Operations
```bash
# Create filesystem
ceph fs new <fs-name> <metadata-pool> <data-pool>
# View filesystem
ceph fs ls
ceph fs status <fs-name>
ceph fs get <fs-name>
# Delete filesystem
ceph fs fail <fs-name>
ceph fs rm <fs-name> --yes-i-really-mean-it
# MDS management
ceph mds stat
ceph mds dump
ceph mds fail <mds-id>
ceph mds repaired <mds-id>
```
### 10.2 CephFS Client
```bash
# Mount CephFS
mount -t ceph <mon-addr>:/ /mnt/cephfs -o name=admin,secret=<key>
mount -t ceph <mon-addr>:/ /mnt/cephfs -o name=admin,secretfile=/etc/ceph/admin.secret
# Use ceph-fuse
ceph-fuse /mnt/cephfs
ceph-fuse /mnt/cephfs -o allow_other
# View client connections
ceph daemon mds.<id> client ls
ceph daemon mds.<id> session ls
# Evict client
ceph daemon mds.<id> client evict id=<client-id>
```
### 10.3 Advanced CephFS Management
```bash
# Subdirectory mounting
mount -t ceph <mon-addr>:/subdir /mnt/subdir -o name=admin
# Configure multiple filesystems
ceph fs flag set enable_multiple true
# Directory quotas
setfattr -n ceph.quota.max_files -v 10000 /mnt/cephfs/dir
setfattr -n ceph.quota.max_bytes -v 1000000000 /mnt/cephfs/dir
getfattr -n ceph.quota.max_files /mnt/cephfs/dir
```
---
## ‚òÅÔ∏è 11. RGW Management
### 11.1 Basic RGW Operations
```bash
# Create user
radosgw-admin user create --uid=<user-id> --display-name="<display-name>"
radosgw-admin user create --uid=testuser --display-name="Test User" --email=test@example.com
# View users
radosgw-admin user list
radosgw-admin user info --uid=<user-id>
# Delete user
radosgw-admin user rm --uid=<user-id>
# Modify user
radosgw-admin user modify --uid=<user-id> --display-name="New Name"
# Create subuser
radosgw-admin subuser create --uid=<user-id> --subuser=<subuser-id> --access=full
```
### 11.2 RGW Key Management
```bash
# Create access key
radosgw-admin key create --uid=<user-id> --key-type=s3
# Delete access key
radosgw-admin key rm --uid=<user-id> --key-type=s3 --access-key=<access-key>
# Generate Swift key
radosgw-admin key create --uid=<user-id> --key-type=swift --gen-secret
```
### 11.3 RGW Maintenance
```bash
# Check bucket index
radosgw-admin bucket check --bucket=<bucket-name>
# Rebuild bucket index
radosgw-admin bucket reshard --bucket=<bucket-name> --num-shards=<num>
# Garbage collection
radosgw-admin gc list
radosgw-admin gc process
# View usage statistics
radosgw-admin usage show --uid=<user-id>
radosgw-admin usage show --show-log-entries=false
# Bucket management
radosgw-admin bucket list
radosgw-admin bucket stats --bucket=<bucket-name>
```
---
## üîß 12. Configuration Management
### 12.1 Runtime Configuration
```bash
# View configuration
ceph config dump
ceph config get <daemon-type>.<daemon-id> <option>
ceph config show <daemon-type>.<daemon-id>
# Set configuration
ceph config set <daemon-type> <option> <value>
ceph config set global osd_pool_default_size 3
ceph config set osd osd_max_backfills 1
# Reset configuration
ceph config rm <daemon-type> <option>
# View configuration history
ceph config log
```
### 12.2 Temporary Configuration Adjustment
```bash
# Temporary configuration (lost after restart)
ceph tell <daemon-type>.<id> config set <option> <value>
ceph tell osd.* config set debug_osd 10
# View configuration
ceph tell <daemon-type>.<id> config show
ceph tell <daemon-type>.<id> config get <option>
# Batch configuration
ceph tell osd.* config set osd_max_backfills 1
ceph tell mon.* config set debug_mon 5
```
---
## üìà 13. Performance Analysis
### 13.1 RBD Performance Analysis
#### RBD Performance Testing Tools
```bash
# Use rbd bench for performance testing
rbd bench --io-type write --io-size 4K --io-threads 16 --io-total 1G <pool>/<image>
rbd bench --io-type read --io-size 4K --io-threads 16 --io-total 1G <pool>/<image>
rbd bench --io-type readwrite --io-size 4K --io-threads 16 --io-total 1G <pool>/<image>
# Use fio for detailed performance testing
fio --name=rbd-test --ioengine=rbd --pool=<pool> --rbdname=<image> \
    --rw=randwrite --bs=4k --iodepth=32 --numjobs=4 --runtime=60 --group_reporting
# Test different block sizes
for bs in 4k 8k 16k 32k 64k 128k; do
    echo "Testing block size: $bs"
    rbd bench --io-type write --io-size $bs --io-threads 16 --io-total 512M <pool>/<image>
done
# Sequential read/write test
fio --name=seq-write --ioengine=rbd --pool=test --rbdname=testimg \
    --rw=write --bs=1M --iodepth=1 --numjobs=1 --runtime=60
```
#### RBD Performance Monitoring
```bash
# Real-time RBD I/O monitoring
rbd perf image iostat
# View RBD image statistics
rbd du <pool>/<image>
rbd info <pool>/<image>
# Monitor client I/O patterns
ceph daemon client.<id> perf dump
ceph daemon client.<id> perf histogram dump
# View RBD cache statistics
ceph daemon client.<id> config show | grep rbd_cache
ceph daemon client.<id> perf dump | grep rbd_cache
```
#### RBD Performance Tuning Parameters
```bash
# Client cache optimization
ceph config set client rbd_cache true
ceph config set client rbd_cache_size 134217728           # 128MB
ceph config set client rbd_cache_max_dirty 100663296      # 96MB
ceph config set client rbd_cache_target_dirty 67108864    # 64MB
ceph config set client rbd_cache_max_dirty_age 10.0
# Readahead optimization
ceph config set client rbd_readahead_trigger_requests 10
ceph config set client rbd_readahead_max_bytes 524288     # 512KB
ceph config set client rbd_readahead_disable_after_bytes 52428800  # 50MB
# Concurrency optimization
ceph config set client rbd_concurrent_management_ops 20
ceph config set client rbd_op_threads 1
```
### 13.2 CephFS Performance Analysis
#### CephFS Performance Testing
```bash
# Basic performance testing with dd
dd if=/dev/zero of=/mnt/cephfs/testfile bs=1M count=1000 oflag=direct
dd if=/mnt/cephfs/testfile of=/dev/null bs=1M iflag=direct
# Comprehensive performance testing with iozone
iozone -a -s 1G -r 4k -r 64k -r 1M -i 0 -i 1 -i 2 -f /mnt/cephfs/testfile
# Small file performance test
mkdir /mnt/cephfs/small_files_test
time for i in {1..10000}; do touch /mnt/cephfs/small_files_test/file_$i; done
# Metadata performance test
time find /mnt/cephfs -name "*.txt" | wc -l
time ls -la /mnt/cephfs/large_directory/ | wc -l
# Concurrent file operations test
for i in {1..10}; do
    (for j in {1..1000}; do
        touch /mnt/cephfs/test_${i}_${j}
    done) &
done
wait
```
#### CephFS Performance Monitoring
```bash
# MDS performance statistics
ceph daemon mds.<id> perf dump
ceph daemon mds.<id> session ls
ceph daemon mds.<id> status
ceph daemon mds.<id> ops
# Client performance statistics
ceph daemon client.<id> perf dump
ceph daemon client.<id> client_metadata
# View MDS load
ceph fs status <fsname>
ceph mds metadata <mds-id>
# Monitor directory fragmentation
ceph daemon mds.<id> dirfrag ls <path>
ceph daemon mds.<id> dirfrag split_info <path>
```
### 13.3 OSD Performance Analysis
#### OSD Performance Benchmarking
```bash
# OSD benchmark testing
ceph tell osd.<id> bench 1073741824 4194304              # 1GB test, 4MB block size
# Direct ceph-osd testing
ceph-osd -i <id> --test-objectstore --test-objectstore-workload uniform_random
# Disk performance testing
dd if=/dev/zero of=/var/lib/ceph/osd/ceph-<id>/test bs=1M count=1000 oflag=direct
dd if=/var/lib/ceph/osd/ceph-<id>/test of=/dev/null bs=1M iflag=direct
# Network performance testing
iperf3 -s    # Run on target OSD node
iperf3 -c <target-osd-ip> -t 30 -P 4    # Run on source node
# Use rados bench for testing
rados bench -p <pool> 60 write --no-cleanup
rados bench -p <pool> 60 seq
rados bench -p <pool> 60 rand
rados -p <pool> cleanup
```
#### Detailed OSD Performance Monitoring
```bash
# OSD performance counters
ceph daemon osd.<id> perf dump
ceph daemon osd.<id> perf histogram dump
# OSD operation statistics
ceph daemon osd.<id> dump_ops_in_flight
ceph daemon osd.<id> dump_blocked_ops
ceph daemon osd.<id> dump_op_pq_state
# OSD storage statistics
ceph daemon osd.<id> status
ceph daemon osd.<id> df
# Slow operation analysis
ceph daemon osd.<id> dump_slow_requests
ceph daemon osd.<id> dump_historic_slow_ops
# OSD recovery statistics
ceph daemon osd.<id> dump_recovery_reservations
ceph daemon osd.<id> dump_backfill_reservations
```
### 13.4 Overall Cluster Performance Analysis
#### Cluster-level Performance Monitoring
```bash
# Cluster I/O statistics
ceph iostat
watch "ceph iostat"
# PG distribution analysis
ceph pg dump | grep ^pg | awk '{print $1,$15}' | sort
# Real-time performance monitoring script
#!/bin/bash
while true; do
    echo "$(date) - IOPS: $(ceph iostat | grep -E 'read|write')"
    echo "$(date) - Health: $(ceph health)"
    sleep 5
done
# Cluster latency analysis
ceph osd perf | grep -E "apply_latency|commit_latency"
```
---
## üõ†Ô∏è 14. Ceph Specialized Tools
### 14.1 Data Recovery and Diagnostic Tools
#### ceph-objectstore-tool (Object Store Tool)
```bash
# List object store contents
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> --op list
# Export PG data
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op export --pgid <pg-id> --file /tmp/pg_export.dat
# Import PG data
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op import --file /tmp/pg_export.dat
# Remove corrupted object
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op remove --pgid <pg-id> --oid <object-id>
# Repair object store
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op fsck --debug
# View object information
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-<id> \
    --op info --pgid <pg-id> --oid <object-id>
```
#### ceph-kvstore-tool (Key-Value Store Tool)
```bash
# List key-value pairs
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> list
# Get specific key-value
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> get <prefix> <key>
# Delete key-value pair
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> rm <prefix> <key>
# Compact database
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> compact
# Backup/restore key-value store
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> backup /tmp/kv_backup
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> restore /tmp/kv_backup
# Statistics
ceph-kvstore-tool bluestore-kv /var/lib/ceph/osd/ceph-<id> stats
```
#### ceph-bluestore-tool (BlueStore Specific Tool)
```bash
# View BlueStore information
ceph-bluestore-tool show-label --dev /dev/sdX
ceph-bluestore-tool prime-osd-dir --dev /dev/sdX --path /var/lib/ceph/osd/ceph-X
# Repair BlueStore
ceph-bluestore-tool fsck --path /var/lib/ceph/osd/ceph-<id>
ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-<id>
# Show BlueStore metadata
ceph-bluestore-tool show-label --dev /dev/sdX
ceph-bluestore-tool show-label --path /var/lib/ceph/osd/ceph-<id>
# BlueStore space analysis
ceph-bluestore-tool bluefs-stats --path /var/lib/ceph/osd/ceph-<id>
ceph-bluestore-tool bluefs-bdev-sizes --path /var/lib/ceph/osd/ceph-<id>
# Allocator tools
ceph-bluestore-tool free-dump --path /var/lib/ceph/osd/ceph-<id>
ceph-bluestore-tool free-score --path /var/lib/ceph/osd/ceph-<id>
```
### 14.2 Monitoring and Management Tools
#### crushtool (CRUSH Mapping Tool)
```bash
# Compile/decompile CRUSH map
ceph osd getcrushmap -o crushmap.bin
crushtool -d crushmap.bin -o crushmap.txt
crushtool -c crushmap.txt -o crushmap_new.bin
ceph osd setcrushmap -i crushmap_new.bin
# Test CRUSH rules
crushtool -i crushmap.bin --test --show-mappings --ruleset 0 --num-rep 3 --min-x 0 --max-x 100
# Analyze CRUSH weights
crushtool -i crushmap.bin --show-choose-tries
# Verify CRUSH map
crushtool -i crushmap.bin --check
# Show CRUSH tree structure
crushtool -i crushmap.bin --tree
```
#### monmaptool (Monitor Mapping Tool)
```bash
# Create Monitor map
monmaptool --create --add <mon-id> <mon-addr> --fsid <uuid> monmap.bin
# View Monitor map
monmaptool --print monmap.bin
# Add/remove Monitor
monmaptool --add <mon-id> <mon-addr> monmap.bin
monmaptool --rm <mon-id> monmap.bin
# Generate new fsid
uuidgen
monmaptool --create --fsid $(uuidgen) monmap.bin
# Extract from existing cluster
ceph mon getmap -o monmap.bin
monmaptool --print monmap.bin
```
#### osdmaptool (OSD Mapping Tool)
```bash
# Export OSD map
ceph osd getmap -o osdmap.bin
osdmaptool osdmap.bin --print
# Test PG mapping
osdmaptool osdmap.bin --test-map-pgs --pool <pool-id>
# Create incremental map
osdmaptool --createsimple 3 osdmap.bin --with-default-pool
# Export specific epoch map
ceph osd getmap <epoch> -o osdmap_<epoch>.bin
# Show OSD tree
osdmaptool osdmap.bin --tree
```
### 14.3 Data Migration and Synchronization Tools
#### rados (Object Storage Client)
```bash
# Basic object operations
rados -p <pool> put <object> <file>
rados -p <pool> get <object> <file>
rados -p <pool> ls
rados -p <pool> rm <object>
# Batch operations
rados -p <pool> ls | xargs -I {} rados -p <pool> rm {}
# Benchmark testing
rados bench -p <pool> 60 write --no-cleanup
rados bench -p <pool> 60 seq
rados bench -p <pool> 60 rand
# Cleanup benchmark data
rados -p <pool> cleanup
# Object attribute operations
rados -p <pool> setxattr <object> <attr> <value>
rados -p <pool> getxattr <object> <attr>
rados -p <pool> listxattr <object>
# Watch object changes
rados -p <pool> watch <object>
```
#### rbd-mirror (RBD Mirror Tool)
```bash
# Enable mirroring
rbd mirror pool enable <pool> pool
rbd mirror pool enable <pool> image
# Configure mirroring
rbd mirror image enable <pool>/<image> snapshot
rbd mirror image enable <pool>/<image> journal
# View mirror status
rbd mirror pool status <pool>
rbd mirror image status <pool>/<image>
# Force resync
rbd mirror image resync <pool>/<image>
# Failover
rbd mirror image promote <pool>/<image>
rbd mirror image demote <pool>/<image>
```
#### ceph-crash (Crash Reporting Tool)
```bash
# View crash reports
ceph crash ls
ceph crash info <crash-id>
# Archive crash reports
ceph crash archive <crash-id>
ceph crash archive-all
# Generate crash report
ceph crash post -i <crash-dump-file>
# Crash statistics
ceph crash stat
```
#### ceph-volume (Volume Management Tool)
```bash
# Create OSD with LVM
ceph-volume lvm create --data /dev/sdb
ceph-volume lvm create --data /dev/sdb --block.wal /dev/nvme0n1p1
# Separate prepare and activate
ceph-volume lvm prepare --data /dev/sdb --osd-id 0
ceph-volume lvm activate 0 <osd-uuid>
# View LVM information
ceph-volume lvm list
ceph-volume lvm list /dev/sdb
# Simple method to create OSD
ceph-volume simple scan /dev/sdb
ceph-volume simple activate <osd-id> <osd-uuid>
```
---
## üö® 15. Troubleshooting
### 15.1 Log Viewing
```bash
# View service logs
journalctl -u ceph-mon@<hostname> -f
journalctl -u ceph-osd@<id> -f
journalctl -u ceph-mds@<hostname> -f
journalctl -u ceph-mgr@<hostname> -f
# View logs by time range
journalctl -u ceph-osd@<id> --since "2023-01-01 00:00:00" --until "2023-01-01 23:59:59"
# Adjust log levels
ceph tell osd.* config set debug_osd 10
ceph tell mon.* config set debug_mon 10
ceph tell mds.* config set debug_mds 10
# View real-time errors
tail -f /var/log/ceph/ceph.log | grep -i error
tail -f /var/log/ceph/ceph-osd.*.log | grep -i slow
```
### 15.2 Common Issue Troubleshooting
```bash
# PG inconsistency issues
ceph health detail | grep inconsistent
ceph pg <pgid> query
ceph pg repair <pgid>
# OSD full issues
ceph osd df | grep -E "(95|96|97|98|99)%"
ceph osd reweight-by-utilization
# Slow request issues
ceph daemon osd.<id> dump_slow_requests
ceph daemon osd.<id> dump_historic_slow_ops
ceph config set osd debug_osd 10
# Clock synchronization issues
ceph time-sync-status
ntpq -p
# Disk space issues
ceph df
ceph osd df
df -h /var/lib/ceph/osd/
```
---
## üîÑ 16. Backup and Recovery
### 16.1 Data Backup
```bash
# Export Monitor map
ceph mon getmap -o monmap.bin
# Export OSD map
ceph osd getmap -o osdmap.bin
# Export CRUSH map
ceph osd getcrushmap -o crushmap.bin
# Backup ceph.conf and keys
cp /etc/ceph/ceph.conf /backup/
cp /etc/ceph/ceph.client.admin.keyring /backup/
# Backup authentication information
ceph auth export > /backup/ceph_auth.txt
# Backup pool information
ceph osd pool ls detail > /backup/pools.txt
ceph pg dump > /backup/pg_dump.txt
```
### 16.2 Disaster Recovery
```bash
# Recover data from single OSD
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-X \
    --export-remove --pgid <pgid> --file /tmp/pg_export
# Rebuild Monitor
ceph-mon --mkfs -i <mon-id> --monmap monmap.bin --keyring mon.keyring
# Restore authentication information
ceph auth import -i /backup/ceph_auth.txt
# Restore CRUSH map
ceph osd setcrushmap -i /backup/crushmap.bin
# OSD data recovery
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-X \
    --op import --file /tmp/pg_export
```
### 16.3 RBD Backup and Recovery
```bash
# RBD snapshot backup
rbd snap create <pool>/<image>@backup-$(date +%Y%m%d)
rbd export <pool>/<image>@<snapshot> /backup/image_backup.raw
# RBD incremental backup
rbd export-diff <pool>/<image>@<snap1> <pool>/<image>@<snap2> /backup/diff.raw
# RBD recovery
rbd import /backup/image_backup.raw <pool>/<new-image>
rbd import-diff /backup/diff.raw <pool>/<image>
```
---
**This document is continuously updated. Please check for the latest version regularly. Any questions or suggestions are welcome.**
*Last updated: June 2025*
